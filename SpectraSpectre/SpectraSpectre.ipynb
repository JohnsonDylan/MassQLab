{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cfcfd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('__________________________')\n",
    "print('')\n",
    "print(\"Initialize Spectra Spectre\")\n",
    "print('__________________________')\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import massql\n",
    "from massql import msql_fileloading, msql_engine\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pyteomics import mzxml, mzml\n",
    "import numpy as np\n",
    "import fnmatch\n",
    "import glob\n",
    "from scipy.integrate import trapz\n",
    "import warnings\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import subprocess\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "starting_directory = os.getcwd()\n",
    "# pyinstaller command\n",
    "# pyinstaller --noconfirm --noupx -F --console --collect-all \"massql\" --collect-all \"matchms\" --collect-all \"pyarrow\" --collect-all \"pymzml\" --exclude-module \"kaleido\"  \"<absolute_path_to_script>\"\n",
    "\n",
    "# Convert jupyter notebok to script\n",
    "# jupyter nbconvert --to script \"<absolute_path_to_notebook>.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e5a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Do not run in Jupyter\"\"\"\n",
    "# if getattr(sys, 'frozen', False):\n",
    "#     bundle_dir = sys._MEIPASS\n",
    "# else:\n",
    "#     bundle_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# if getattr(sys, 'frozen', False) and hasattr(sys, '_MEIPASS'):\n",
    "#     print('Running in a PyInstaller bundle')\n",
    "# else:\n",
    "#     print('Running in a normal Python process')\n",
    "# print('')\n",
    "# print( 'bundle dir is', bundle_dir )\n",
    "# print( 'sys.argv[0] is', sys.argv[0] )\n",
    "# print( 'sys.executable is', sys.executable )\n",
    "# print( 'os.getcwd is', os.getcwd() )\n",
    "# print('__________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441662d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Configure\"\"\"\n",
    "try:\n",
    "    with open(\"spectre_config.json\") as config_file:\n",
    "        config = json.load(config_file)\n",
    "        use_queryfile = config['use_queryfile']\n",
    "        queryfile = config['queryfile']\n",
    "        print(\"Query File Excel: \"+str(queryfile))\n",
    "        use_queryfilejson = config['use_queryfile_json']\n",
    "        queryfilejson = config['queryfile_json']\n",
    "        print(\"Query File JSON: \"+str(queryfilejson))\n",
    "        cache_setting = config['cache']\n",
    "        print(\"Use Cache: \"+str(cache_setting))\n",
    "        datasaver = config['datasaver']\n",
    "        print(\"Use Datasaver: \"+str(datasaver))\n",
    "        data_directory = config['data_directory']\n",
    "        print(\"Data Directory: \"+str(data_directory))\n",
    "        QC_files = config['QC_files']\n",
    "        kegg_path = config['kegg_path']\n",
    "        convert_raw = config['convert_raw']\n",
    "        print(\"Convert raw: \"+str(convert_raw))\n",
    "        msconvertexe = config['msconvert_exe']\n",
    "        if convert_raw:\n",
    "            print(\"MSConvert exe: \"+str(msconvertexe))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"FileNotFoundError\\n\"\n",
    "          f\"{e} \\n\"\n",
    "          f\"Not found in \"+os.getcwd()+\"\\n\")\n",
    "    input(\"Press enter to exit...\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b38d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Definition Used to Generate a Query from Excel\"\"\"\n",
    "def create_query(name, KEGG, MS1_MZ, MS1_MZ_tolerance_ppm, rtmin, rtmax):\n",
    "    query = \"QUERY scaninfo(MS1DATA) FILTER MS1MZ=\"+     str(MS1_MZ)+\":TOLERANCEPPM=\"+     str(MS1_MZ_tolerance_ppm)+     \" AND RTMIN=\"+(str(rtmin))+     \" AND RTMAX=\"+str(rtmax)\n",
    "    return {'name':name, 'KEGG': KEGG, 'query':query, 'rtmin': str(rtmin), 'rtmax': str(rtmax)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78188c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Create Queries from Query File\"\"\"\n",
    "queries_excel = []\n",
    "name_kegg_dict = {}\n",
    "MassQL_query_df = pd.DataFrame()\n",
    "if use_queryfile:\n",
    "    if queryfile:\n",
    "        try: \n",
    "            MassQL_query_df = pd.read_excel(queryfile)\n",
    "            print(\"\\nLoaded MassQL queries from: \"+str(queryfile)+\"\\n\")\n",
    "            for index, row in MassQL_query_df.iterrows():\n",
    "                if row['ion_mode'] == 1:\n",
    "                    MS1MZ = row['Monoisotopic'] + 1.00725\n",
    "                else:\n",
    "                    MS1MZ = row['Monoisotopic'] - 1.00725\n",
    "                queries_excel.append(create_query(row['Name'], row['KEGG'], MS1MZ, row['TOLERANCEPPM'], row['RTMIN'], row['RTMAX']))\n",
    "                name_kegg_dict.update({row['Name']: row['KEGG']})\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"FileNotFoundError\\n\"\n",
    "                  f\"{e} \\n\"\n",
    "                  f\"Not found in \"+os.getcwd()+\"\\n\")\n",
    "\n",
    "queries_json = []\n",
    "if use_queryfilejson:\n",
    "    if queryfilejson:\n",
    "        try:\n",
    "            with open(queryfilejson) as queryfilej:\n",
    "                queryjson = json.load(queryfilej)\n",
    "                print(\"\\nLoaded MassQL queries from: \"+str(queryfilejson)+\"\\n\")\n",
    "                for entry in queryjson:\n",
    "                    queries_json.append(entry)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"FileNotFoundError\\n\"\n",
    "                  f\"{e} \\n\"\n",
    "                  f\"Not found in \"+os.getcwd()+\"\\n\")\n",
    "        \n",
    "queries = queries_excel + queries_json\n",
    "if queries:\n",
    "    print(\"\\nCreated \" + str(len(queries)) + \" MassQL Queries\")\n",
    "else:\n",
    "    print(\"No Queries Found\")\n",
    "    input(\"Press enter to exit...\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414ef76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Override MassQL definition to add datasaver function\"\"\"\n",
    "def custom_load_data_mzML_pyteomics(input_filename, datasaver=datasaver):\n",
    "    \"\"\"\n",
    "    This is a loading operation using pyteomics to help with loading mzML files with ion mobility\n",
    "    Args:\n",
    "        input_filename ([type]): [description]\n",
    "    \"\"\"\n",
    "\n",
    "    previous_ms1_scan = 0\n",
    "\n",
    "    # MS1\n",
    "    all_mz = []\n",
    "    all_rt = []\n",
    "    all_polarity = []\n",
    "    all_i = []\n",
    "    all_i_norm = []\n",
    "    all_i_tic_norm = []\n",
    "    all_scan = []\n",
    "\n",
    "    # MS2\n",
    "    all_msn_mz = []\n",
    "    all_msn_rt = []\n",
    "    all_msn_polarity = []\n",
    "    all_msn_i = []\n",
    "    all_msn_i_norm = []\n",
    "    all_msn_i_tic_norm = []\n",
    "    all_msn_scan = []\n",
    "    all_msn_precmz = []\n",
    "    all_msn_ms1scan = []\n",
    "    all_msn_charge = []\n",
    "    all_msn_mobility = []\n",
    "\n",
    "    with mzml.read(input_filename) as reader:\n",
    "        for spectrum in tqdm(reader):\n",
    "\n",
    "            if len(spectrum[\"intensity array\"]) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Getting the RT\n",
    "            try:\n",
    "                rt = spectrum[\"scanList\"][\"scan\"][0][\"scan start time\"]\n",
    "            except:\n",
    "                rt = 0\n",
    "            \n",
    "            # Correcting the unit\n",
    "            try:\n",
    "                if spectrum[\"scanList\"][\"scan\"][0][\"scan start time\"].unit_info == \"second\":\n",
    "                    rt = rt / 60\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            scan = int(spectrum[\"id\"].replace(\"scanId=\", \"\").split(\"scan=\")[-1])\n",
    "            \n",
    "            if not \"m/z array\" in spectrum:\n",
    "                # This is not a mass spectrum\n",
    "                continue\n",
    "\n",
    "            mzi_np = np.column_stack((spectrum[\"m/z array\"], spectrum[\"intensity array\"]))\n",
    "            if datasaver:\n",
    "                # if np.any(mzi_np > 0, axis=1):\n",
    "                mzi_np = np.delete(mzi_np, np.where(mzi_np[:, 1] == 0), axis=0)\n",
    "\n",
    "            mz, intensity = mzi_np.T\n",
    "            i_max = max(intensity)\n",
    "            i_sum = sum(intensity)\n",
    "\n",
    "            # If there is no ms level, its likely an UV/VIS spectrum and we can skip\n",
    "            if not \"ms level\" in spectrum:\n",
    "                continue\n",
    "            \n",
    "            mslevel = spectrum[\"ms level\"]\n",
    "            if mslevel == 1:\n",
    "                all_mz += list(mz)\n",
    "                all_i += list(intensity)\n",
    "                all_i_norm += list(intensity / i_max)\n",
    "                all_i_tic_norm += list(intensity / i_sum)\n",
    "                all_rt += len(mz) * [rt]\n",
    "                all_scan += len(mz) * [scan]\n",
    "                all_polarity += len(mz) * [msql_fileloading._determine_scan_polarity_pyteomics_mzML(spectrum)]\n",
    "\n",
    "                previous_ms1_scan = scan\n",
    "\n",
    "            if mslevel == 2:\n",
    "                msn_mz = spectrum[\"precursorList\"][\"precursor\"][0][\"selectedIonList\"][\"selectedIon\"][0][\"selected ion m/z\"]\n",
    "                msn_charge = 0\n",
    "\n",
    "                if \"charge state\" in spectrum[\"precursorList\"][\"precursor\"][0][\"selectedIonList\"][\"selectedIon\"][0]:\n",
    "                    msn_charge = int(spectrum[\"precursorList\"][\"precursor\"][0][\"selectedIonList\"][\"selectedIon\"][0][\"charge state\"])\n",
    "\n",
    "                all_msn_mz += list(mz)\n",
    "                all_msn_i += list(intensity)\n",
    "                all_msn_i_norm += list(intensity / i_max)\n",
    "                all_msn_i_tic_norm += list(intensity / i_sum)\n",
    "                all_msn_rt += len(mz) * [rt]\n",
    "                all_msn_scan += len(mz) * [scan]\n",
    "                all_msn_polarity += len(mz) * [msql_fileloading._determine_scan_polarity_pyteomics_mzML(spectrum)]\n",
    "                all_msn_precmz += len(mz) * [msn_mz]\n",
    "                all_msn_ms1scan += len(mz) * [previous_ms1_scan] \n",
    "                all_msn_charge += len(mz) * [msn_charge]\n",
    "\n",
    "                if \"product ion mobility\" in spectrum[\"precursorList\"][\"precursor\"][0][\"selectedIonList\"][\"selectedIon\"][0]:\n",
    "                    mobility = spectrum[\"precursorList\"][\"precursor\"][0][\"selectedIonList\"][\"selectedIon\"][0][\"product ion mobility\"]\n",
    "                    all_msn_mobility += len(mz) * [mobility]\n",
    "\n",
    "    ms1_df = pd.DataFrame()\n",
    "    if len(all_mz) > 0:\n",
    "        ms1_df['i'] = all_i\n",
    "        ms1_df['i_norm'] = all_i_norm\n",
    "        ms1_df['i_tic_norm'] = all_i_tic_norm\n",
    "        ms1_df['mz'] = all_mz\n",
    "        ms1_df['scan'] = all_scan\n",
    "        ms1_df['rt'] = all_rt\n",
    "        ms1_df['polarity'] = all_polarity\n",
    "        if datasaver:\n",
    "            for cat_col in ['scan', 'polarity']:\n",
    "                ms1_df = ms1_df[ms1_df[cat_col].notnull()].copy()\n",
    "                # ms1_df.loc[:, cat_col] = ms1_df[cat_col].astype('category')\n",
    "            # ms1_df['scan'] = ms1_df['scan'].astype('category')\n",
    "            # ms1_df['polarity'] = ms1_df['polarity'].astype('category')\n",
    "\n",
    "    ms2_df = pd.DataFrame()\n",
    "    if len(all_msn_mz) > 0:\n",
    "        ms2_df['i'] = all_msn_i\n",
    "        ms2_df['i_norm'] = all_msn_i_norm\n",
    "        ms2_df['i_tic_norm'] = all_msn_i_tic_norm\n",
    "        ms2_df['mz'] = all_msn_mz\n",
    "        ms2_df['scan'] = all_msn_scan\n",
    "        ms2_df['rt'] = all_msn_rt\n",
    "        ms2_df[\"polarity\"] = all_msn_polarity\n",
    "        ms2_df[\"precmz\"] = all_msn_precmz\n",
    "        ms2_df[\"ms1scan\"] = all_msn_ms1scan\n",
    "        ms2_df[\"charge\"] = all_msn_charge\n",
    "        if datasaver:\n",
    "            for cat_col in ['scan', 'polarity']:\n",
    "                ms2_df = ms2_df[ms2_df[cat_col].notnull()].copy()\n",
    "                # ms2_df.loc[:, cat_col] = ms2_df[cat_col].astype('category')\n",
    "        if len(all_msn_mobility) == len(all_msn_i):\n",
    "            ms2_df[\"mobility\"] = all_msn_mobility\n",
    "    \n",
    "    return ms1_df, ms2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e24bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"MassQL file loading\"\"\"\n",
    "def mq_load_data(input_filename, cache=False):\n",
    "    \"\"\"\n",
    "    Loading data generically\n",
    "    Args:\n",
    "        input_filename ([type]): [description]\n",
    "        cache (bool, optional): [description]. Defaults to False.\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    if cache:\n",
    "        ms1_filename = input_filename + \"_ms1.msql.feather\"\n",
    "        ms2_filename = input_filename + \"_ms2.msql.feather\"\n",
    "\n",
    "        # if ms1_filename in config_path or ms2_filename in config_path:\n",
    "\n",
    "        if os.path.exists(ms1_filename) or os.path.exists(ms2_filename):\n",
    "            try:\n",
    "                ms1_df = pd.read_feather(ms1_filename)\n",
    "            except:\n",
    "                ms1_df = pd.DataFrame()\n",
    "            try:\n",
    "                ms2_df = pd.read_feather(ms2_filename)\n",
    "            except:\n",
    "                ms2_df = pd.DataFrame()\n",
    "\n",
    "            return ms1_df, ms2_df\n",
    "\n",
    "    # Actually loading\n",
    "    if input_filename[-5:].lower() == \".mzml\":\n",
    "        #ms1_df, ms2_df = _load_data_mzML(input_filename)\n",
    "        #ms1_df, ms2_df = _load_data_mzML2(input_filename) # Faster version using pymzML\n",
    "        ms1_df, ms2_df = custom_load_data_mzML_pyteomics(input_filename) # Faster version using pymzML\n",
    "\n",
    "    elif input_filename[-6:].lower() == \".mzxml\":\n",
    "        ms1_df, ms2_df = msql_fileloading._load_data_mzXML(input_filename)\n",
    "    \n",
    "    elif input_filename[-5:] == \".json\":\n",
    "        ms1_df, ms2_df = msql_fileloading._load_data_gnps_json(input_filename)\n",
    "    \n",
    "    elif input_filename[-4:].lower() == \".mgf\":\n",
    "        ms1_df, ms2_df = msql_fileloading._load_data_mgf(input_filename)\n",
    "\n",
    "    elif input_filename[-4:].lower() == \".txt\" or input_filename[-4:].lower() == \".dat\":\n",
    "        ms1_df, ms2_df = msql_fileloading._load_data_txt(input_filename)\n",
    "    \n",
    "    else:\n",
    "        print(\"Cannot Load File Extension\")\n",
    "        raise Exception(\"File Format Not Supported\")\n",
    "\n",
    "\n",
    "    # Saving Cache\n",
    "    if cache:\n",
    "        ms1_filename = input_filename + \"_ms1.msql.feather\"\n",
    "        ms2_filename = input_filename + \"_ms2.msql.feather\"\n",
    "\n",
    "        if not (os.path.exists(ms1_filename) or os.path.exists(ms2_filename)):\n",
    "            try:\n",
    "                ms1_df.to_feather(ms1_filename)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                ms2_df.to_feather(ms2_filename)\n",
    "            except:\n",
    "                pass\n",
    "    return ms1_df, ms2_df\n",
    "\n",
    "print(\"\")\n",
    "os.chdir(data_directory)\n",
    "print('Current working directory is now data directory: '+os.getcwd())\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f562a02-860e-450e-89b4-4c666ec45152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Convert raw files\"\"\"\n",
    "path = os.getcwd()\n",
    "convert_count = 0\n",
    "if convert_raw and msconvertexe:\n",
    "    try:\n",
    "        subprocess.run(msconvertexe, \n",
    "                       stdout=subprocess.DEVNULL, \n",
    "                       stderr=subprocess.STDOUT, \n",
    "                       creationflags=subprocess.CREATE_NO_WINDOW)\n",
    "        for fn in os.listdir(path):\n",
    "            if \".raw\" in fn:\n",
    "                if os.path.isfile(path + '\\\\' + fn.replace('.raw','.mzML')):\n",
    "                    pass\n",
    "                    # print (path +\"\\\\\"+ fn + \" already exists\")\n",
    "                else:\n",
    "                    print (path + '\\\\' + fn + \" not converted yet!!\") \n",
    "                    subprocess.run(msconvertexe + \" \" + fn +  \" --zlib\", \n",
    "                                  stdout=subprocess.DEVNULL,\n",
    "                                   stderr=subprocess.STDOUT,\n",
    "                                   creationflags=subprocess.CREATE_NO_WINDOW)\n",
    "                    print(msconvertexe + \" \" + fn +  \" --zlib\")\n",
    "                    if os.path.isfile(path + '\\\\' + fn.replace('.raw','.mzML')):\n",
    "                        print (path + '\\\\' + fn + \" conversion completed!\")\n",
    "                        convert_count += 1\n",
    "                    else:\n",
    "                        print (path + '\\\\' + fn + \" conversion FAILED!\")\n",
    "    except Exception:\n",
    "        print(\"Path to MSConvert is Invalid. No raw files will be converted.\")\n",
    "else:\n",
    "    print(\"Not converting raw files (if any)\")\n",
    "\n",
    "if convert_count == 0:\n",
    "    print(\"No files converted\")\n",
    "elif convert_count > 0:\n",
    "    print(str(convert_count) + \" file(s) converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f93fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Query files\"\"\"\n",
    "peak_area_df = pd.DataFrame()\n",
    "raw_df = pd.DataFrame()\n",
    "all_results_list = []\n",
    "try:\n",
    "    # file_count = len(fnmatch.filter(os.listdir(\"DataMZML\\\\\"), '*.mzml'))\n",
    "    file_count = len(fnmatch.filter(os.listdir(), '*.mzml'))\n",
    "\n",
    "    if file_count == 0:\n",
    "        # print('No mzml files found in '+os.getcwd()+\"\\\\DataMZML\\\\\\n\")\n",
    "        print('No mzml files found in '+os.getcwd()+\"\\\\\\n\")\n",
    "\n",
    "        input(\"Press enter to exit...\")\n",
    "        exit()\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"FileNotFoundError\\n\"\n",
    "          f\"{e} \\n\"\n",
    "          f\"Not found in \"+os.getcwd()+\"\\n\")\n",
    "    input(\"Press enter to exit...\")\n",
    "    exit()\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "counter = 0\n",
    "for filepath in sorted(glob.iglob('*.mzML')):\n",
    "    counter += 1\n",
    "    print('')\n",
    "    print('----- Processing File '+str(counter)+' of '+str(file_count)+' -----')\n",
    "    filename = filepath\n",
    "    ms1_df, ms2_df = mq_load_data(filepath, cache=cache_setting)\n",
    "    for i, query in enumerate(queries):\n",
    "        int_range = float(query['rtmax']) - float(query['rtmin'])\n",
    "        results_df = pd.DataFrame()\n",
    "        try:\n",
    "            results_df = msql_engine.process_query(query['query'], filepath, cache=cache_setting, ms1_df=ms1_df, ms2_df=ms2_df)\n",
    "\n",
    "        except Exception:\n",
    "            print('Query failure\\n' + 'File: ' + str(filename) + '\\nQuery: ' + str(query['query']))\n",
    "            print('Query will fail if running MS2 query on file without MS2 data')\n",
    "            pass \n",
    "        raw_massql_df = results_df.copy()\n",
    "        raw_massql_df['filename'] = filename\n",
    "        raw_massql_df['query'] = query['name']\n",
    "        raw_df = pd.concat([raw_df, raw_massql_df], ignore_index=True)\n",
    "        raw_massql_df = pd.DataFrame()\n",
    "        if not results_df.empty and 'rt' in results_df.columns and len(results_df) > 0:\n",
    "            # results_df = results_df.loc[(results_df['rt'] > float(query['rtmin'])-(int_range/2)) & (results_df.rt<float(query['rtmax'])+(int_range/2))]\n",
    "            results_df_i = results_df.loc[(results_df['rt'] > float(query['rtmin'])) & (results_df.rt<float(query['rtmax']))].copy()\n",
    "            if len(results_df_i) > 0:\n",
    "                if len(results_df_i) > 1:\n",
    "                    peak_area = trapz(results_df_i.i, x=results_df_i.rt)\n",
    "                else:\n",
    "                    peak_area = sum(results_df_i.loc[:,\"i\"])\n",
    "            else:\n",
    "                peak_area = 0\n",
    "            results_df_i = pd.DataFrame()\n",
    "            peak_area_df.at[filename, 'file_directory'] = os.getcwd()\n",
    "            peak_area_df.at[filename, query['name']] = peak_area\n",
    "            results_df.loc[:, \"query_name\"] = query['name']\n",
    "            results_df.loc[:, \"file\"] = os.getcwd()+\"\\\\\"+filepath\n",
    "            results_df.loc[:, \"file_directory\"] = os.getcwd()\n",
    "            results_df.loc[:, \"filename\"] = filename\n",
    "            if datasaver:\n",
    "                for cat_col in ['mslevel', 'query_name', 'file', 'file_directory', 'filename']:\n",
    "                    results_df = results_df[results_df[cat_col].notnull()].copy()\n",
    "                    # results_df.loc[:, cat_col] = results_df[cat_col].astype('category')\n",
    "            all_results_list.append(results_df)\n",
    "        else:\n",
    "            peak_area_df.at[filename, 'file_directory'] = os.getcwd()\n",
    "            peak_area_df.at[filename, query['name']] = 0\n",
    "if all_results_list:        \n",
    "    results_df = pd.concat(all_results_list)\n",
    "# print(results_df.memory_usage(index=True, deep=True).sum()/1000000000)\n",
    "\n",
    "if results_df.empty:\n",
    "    print('\\nNo matches for any query in any datafile\\n')\n",
    "    input(\"Press enter to exit...\")\n",
    "    exit()\n",
    "\n",
    "# display(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476c24b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Integrate and Plot Results\"\"\"\n",
    "for i, query in enumerate(queries):\n",
    "    int_range = float(query['rtmax']) - float(query['rtmin'])\n",
    "    fig1 = plt.figure(figsize=(12,8))\n",
    "    plt.subplots_adjust(bottom=0.3, top=.9, wspace = .1)\n",
    "    fig1.suptitle(query['name'], y = .96, fontsize=16)\n",
    "    fig1_sub1 = fig1.add_subplot(121, title='Intensity vs RT', xlabel='retention time', ylabel='intensity')\n",
    "    fig1_sub1.title.set_size(14)\n",
    "    fig1_sub1.axvline(x=float(query['rtmin']), color='b')\n",
    "    fig1_sub1.axvline(x=float(query['rtmax']), color='b')\n",
    "    fig1_sub1.axvline(x=float(query['rtmin']), color='r', linestyle='--')\n",
    "    fig1_sub1.axvline(x=float(query['rtmax']), color='r', linestyle='--')\n",
    "    fig1_sub1.set_xlim([float(query['rtmin'])-(int_range/2), float(query['rtmax'])+(int_range/2)])\n",
    "    fig1_sub1.set_ylabel('Intensity', fontsize=12)\n",
    "    fig1_sub1.set_xlabel('Retention Time', fontsize=12)\n",
    "    fig1_sub2 = fig1.add_subplot(122, title='peak area')\n",
    "    # fig1_sub2.set_xlabel('file', fontsize=12)\n",
    "    fig1_sub2.set_ylabel('peak area', fontsize=12)\n",
    "    fig1_sub2.title.set_size(14)\n",
    "    fig1_sub2.tick_params('x', labelrotation=90, labelsize=8)\n",
    "    fig1_sub2.yaxis.tick_right()\n",
    "    fig1_sub2.yaxis.set_label_position(\"right\")\n",
    "    fig1_sub2.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
    "    fig1_sub2.ticklabel_format(axis='x', style='sci', scilimits=(0, 0))\n",
    "    plt.subplots_adjust(bottom=0.3, top=.9, wspace = .1, left = 0.075)\n",
    "    for file_n in results_df['file'].unique():\n",
    "        file_directory, filename = file_n.rsplit('\\\\', 1)\n",
    "        filtered_data = results_df.loc[(results_df['query_name']==query['name']) & (results_df['file']==file_n)].copy()\n",
    "        peak_area = peak_area_df.loc[filename][query['name']]\n",
    "        fig1_sub1.plot(filtered_data.rt, filtered_data.i)\n",
    "        fig1_sub2.scatter(filename, peak_area)\n",
    "    ratio = 1.0\n",
    "    x_left, x_right = fig1_sub1.get_xlim()\n",
    "    y_low, y_high = fig1_sub1.get_ylim()\n",
    "    fig1_sub1.set_aspect(abs((x_right-x_left)/(y_low-y_high))*ratio)\n",
    "    x_left, x_right = fig1_sub2.get_xlim()\n",
    "    y_low, y_high = fig1_sub2.get_ylim()\n",
    "    fig1_sub2.set_aspect(abs((x_right-x_left)/(y_low-y_high))*ratio)\n",
    "\n",
    "\"\"\"Save Results to Files\"\"\"\n",
    "timestr = time.strftime(\"%Y_%m_%d_%H%M\")\n",
    "def save_image(filename):\n",
    "    p = PdfPages(filename)\n",
    "    fig_nums = plt.get_fignums()\n",
    "    figs = [plt.figure(n) for n in fig_nums]\n",
    "    for fig in figs: \n",
    "        fig.savefig(p, format='pdf') \n",
    "    p.close()  \n",
    "\n",
    "if not os.path.exists(\"SpectraSpectre_Output/\"+timestr):\n",
    "    os.makedirs(\"SpectraSpectre_Output/\"+timestr)\n",
    "pdf_filename = \"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_images.pdf\"  \n",
    "save_image(pdf_filename)\n",
    "\n",
    "# peak_area_df_new = peak_area_df.reset_index(names=['CORE_Filename'])\n",
    "peak_area_df_new = peak_area_df.reset_index()\n",
    "peak_area_df_new = peak_area_df_new.rename(columns = {'index':'CORE_Filename'})\n",
    "\n",
    "raw_df.to_csv(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_raw_query_df.csv\")  \n",
    "\n",
    "with pd.ExcelWriter(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_results.xlsx\") as writer:\n",
    "    peak_area_df_new.to_excel(writer, sheet_name=\"results\", index=False)\n",
    "    if not MassQL_query_df.empty:\n",
    "        MassQL_query_df.to_excel(writer, sheet_name=\"queries\", index=False)\n",
    "\n",
    "    \n",
    "    \n",
    "peak_area_df_biopan = peak_area_df_new.drop(columns=['file_directory'])\n",
    "\n",
    "def remove_filename_ext(filenameext):\n",
    "    filenameext = str(filenameext)  # cast to string\n",
    "    filenamenoext = filenameext[:-5] # remove last five characters\n",
    "    return str(filenamenoext)\n",
    "\n",
    "peak_area_df_biopan['CORE_Filename'] =peak_area_df_biopan['CORE_Filename'].apply(remove_filename_ext)\n",
    "\n",
    "peak_area_df_biopan.set_index('CORE_Filename',inplace=True)\n",
    "peak_area_df_biopan = peak_area_df_biopan.T\n",
    "peak_area_df_biopan.to_csv(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_results_biopan.csv\")  \n",
    "\n",
    "print(\"\\nResults saved to:\")\n",
    "print(os.getcwd()+\"\\\\SpectraSpectre_Output\\n\")\n",
    "# print('Complete\\n')\n",
    "# input(\"Press enter to exit...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e3703b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "QC_df = peak_area_df_new[peak_area_df_new['CORE_Filename'].str.startswith(tuple(QC_files)) | peak_area_df_new['CORE_Filename'].str.startswith('QC_')]\n",
    "# QC_df = peak_area_df_new[peak_area_df_new['CORE_Filename'].str.startswith('QC_')]\n",
    "if QC_df.empty:\n",
    "    print(\"No System Suitability Check\\n\")\n",
    "elif not MassQL_query_df:\n",
    "    print(\"No System Suitability Check\\n\")\n",
    "else:\n",
    "    peak_area_df_QC1 = peak_area_df_new.copy()\n",
    "    qc_dict = {'CORE_Filename': 'QC_Average'}\n",
    "\n",
    "    for index, row in MassQL_query_df.iterrows():\n",
    "        qname = row['Name']\n",
    "        qave = QC_df.loc[:, qname].mean()\n",
    "        qc_dict.update({qname:qave})\n",
    "\n",
    "    qc_row = pd.Series(qc_dict)\n",
    "    peak_area_df_QC1 = pd.concat([peak_area_df_QC1, qc_row.to_frame().T], ignore_index=True)\n",
    "    peak_area_df_QC1 = peak_area_df_QC1.set_index('CORE_Filename')\n",
    "    peak_area_df_QC1 = peak_area_df_QC1.drop('file_directory', axis=1)\n",
    "    peak_area_df_QC2 = peak_area_df_QC1.copy()\n",
    "\n",
    "    peak_area_df_QC2.loc[\"2022MCF0031_p_QCcurve_02_02.mzML\",\"propionyl carnitine-d3\"] = 1001\n",
    "\n",
    "    for index, row in MassQL_query_df.iterrows():\n",
    "        qname = row['Name']\n",
    "        qthreshold = row['QC_threshold']\n",
    "        qave = QC_df.loc[:, qname].mean()\n",
    "        peak_area_df_QC2[qname] = peak_area_df_QC2[qname].apply(lambda x: 1 if qave==x else (None if qave == 0 else x/qave))\n",
    "\n",
    "    peak_area_df_QC3 = peak_area_df_QC2.copy()\n",
    "\n",
    "    for index, row in MassQL_query_df.iterrows():\n",
    "        qname = row['Name']\n",
    "        qthreshold = row['QC_threshold']\n",
    "        peak_area_df_QC3[qname] = peak_area_df_QC3[qname].apply(lambda x: True if math.isclose(1,x,abs_tol=qthreshold) else False)\n",
    "\n",
    "    peak_area_df_QC = peak_area_df_QC3.copy()\n",
    "    peak_area_df_QC['System_Suitability'] = peak_area_df_QC.all(axis=1)\n",
    "    # peak_area_df_QC = peak_area_df_QC[~peak_area_df_QC.index.str.startswith('QC_')]\n",
    "    peak_area_df_QC = peak_area_df_QC[~(peak_area_df_QC.index.str.startswith(tuple(QC_files)) | peak_area_df_QC.index.str.startswith('QC_'))]\n",
    "    peak_area_df_QC = peak_area_df_QC.loc[:, peak_area_df_QC.columns.str.startswith('System_Suitability')]\n",
    "\n",
    "    peak_area_df_QC = peak_area_df_QC.reset_index()\n",
    "    peak_area_df_QC1 = peak_area_df_QC1.reset_index()\n",
    "    peak_area_df_QC2 = peak_area_df_QC2.reset_index()\n",
    "    peak_area_df_QC3 = peak_area_df_QC3.reset_index()\n",
    "\n",
    "    with pd.ExcelWriter(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_QC.xlsx\") as writer:\n",
    "        peak_area_df_QC.to_excel(writer, sheet_name=\"QC\", index=False)\n",
    "        peak_area_df_QC1.to_excel(writer, sheet_name=\"QC1\", index=False)\n",
    "        peak_area_df_QC2.to_excel(writer, sheet_name=\"QC2\", index=False)\n",
    "        peak_area_df_QC3.to_excel(writer, sheet_name=\"QC3\", index=False)\n",
    "\n",
    "    print('System Suitability Results:\\n')\n",
    "    # print(QC_df['CORE_Filename'].values.tolist())\n",
    "    print('QC Files: ' + str(QC_df['CORE_Filename'].values.tolist()))\n",
    "    print('\\n')\n",
    "    print(peak_area_df_QC)\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5948ae3e-5bd9-47d2-bcbb-792d4e8e52f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if kegg_path:\n",
    "    # os.chdir(starting_directory)\n",
    "    import Bio\n",
    "    Bio.__version__\n",
    "\n",
    "    from Bio import SeqIO\n",
    "    from Bio.KEGG.REST import *\n",
    "    from Bio.KEGG.KGML import KGML_parser\n",
    "    from Bio.Graphics.KGML_vis import KGMLCanvas\n",
    "    from Bio.Graphics.ColorSpiral import ColorSpiral\n",
    "    from IPython.display import Image, HTML\n",
    "    import random\n",
    "    from pdf2image import convert_from_path\n",
    "    from IPython.display import Image \n",
    "\n",
    "    # A bit of code that will help us display the PDF output\n",
    "    def PDF(filename):\n",
    "        return HTML('<iframe src=%s width=700 height=350></iframe>' % filename)\n",
    "\n",
    "    # A bit of helper code to shorten long text\n",
    "    def head(text, lines=10):\n",
    "        \"\"\" Print the first lines lines of the passed text.\"\"\"\n",
    "        print('\\n'.join(text.split('\\n')[:lines] + ['[...]']))\n",
    "\n",
    "    def rgb_to_hex(rgb):\n",
    "        rgb = tuple([int(val) for val in rgb])\n",
    "        return '#' + ''.join([hex(val)[2:] for val in rgb]).upper()\n",
    "\n",
    "    peak_area_df_transform = peak_area_df_new.copy()\n",
    "    def transform_func(number):\n",
    "        return np.log2(1+number)\n",
    "\n",
    "    # executing the function\n",
    "    bad_column = [\"CORE_Filename\", \"file_directory\"]\n",
    "    other_cols = peak_area_df_transform.columns.difference(bad_column)\n",
    "    peak_area_df_transform[other_cols] = peak_area_df_transform[other_cols].apply(transform_func)\n",
    "    peak_area_df_transform[other_cols]= peak_area_df_transform[other_cols].div(peak_area_df_transform[other_cols].iloc[0])\n",
    "\n",
    "    # displaying the DataFrame\n",
    "    # display(peak_area_df_transform)\n",
    "\n",
    "    shade_dict = {}\n",
    "    for index, row in peak_area_df_transform[other_cols].iterrows():\n",
    "        if index == 0:\n",
    "            pass\n",
    "        else:\n",
    "            for col_name in peak_area_df_transform[other_cols].columns:\n",
    "                # print(f\"{col_name}: {row[col_name]}\")\n",
    "                if col_name in name_kegg_dict.keys():\n",
    "                    cid = str(name_kegg_dict[col_name])\n",
    "                    cid = cid.replace(u'\\xa0', u'')\n",
    "                    if row[col_name] > 1.1:\n",
    "                        mer = rgb_to_hex([255,255*(1/row[col_name]),255*(1/row[col_name])])\n",
    "                    elif row[col_name] <= 1.1 and row[col_name] >= 0.9:\n",
    "                        mer = rgb_to_hex([255,255,255])\n",
    "                    elif row[col_name] > 0 and row[col_name] < 0.9:\n",
    "                        mer = rgb_to_hex([255*(row[col_name]),255*(row[col_name]),255])\n",
    "                    else:\n",
    "                        mer = rgb_to_hex([150,150,150])\n",
    "                    shade_dict.update({str(cid):mer})\n",
    "\n",
    "                    # from reportlab.lib.colors import HexColor\n",
    "    # pathway = KGML_parser.read(kegg_get(\"hsa00020\", \"kgml\"))\n",
    "    try:\n",
    "        pathway = KGML_parser.read(kegg_get(kegg_path, \"kgml\"))\n",
    "\n",
    "        print(pathway)\n",
    "\n",
    "        for x in pathway.compounds:\n",
    "            # print(str(shade_dict[x.graphics[0].name]))\n",
    "            # print(shade_dict)\n",
    "            try:\n",
    "                x.graphics[0].bgcolor = str(shade_dict[x.graphics[0].name]) + str('CC')\n",
    "                # x.graphics[0].fgcolor = str(shade_dict[x.graphics[0].name])\n",
    "                if shade_dict[x.graphics[0].name] == '#FFFFFF' or shade_dict[x.graphics[0].name] == \"#969696\":   \n",
    "                    x.graphics[0].fgcolor = \"#000000\" + str('66')\n",
    "                else:\n",
    "                    x.graphics[0].fgcolor = str(shade_dict[x.graphics[0].name]) + str('CC')\n",
    "            except Exception:\n",
    "                pass\n",
    "                # print('no match')\n",
    "            x.graphics[0].type=\"circle\"\n",
    "            # x.graphics[0].fgcolor = \"#d1d1d1\"\n",
    "            x.graphics[0].width = 30\n",
    "            x.graphics[0].height = 17\n",
    "\n",
    "        for x in pathway.orthologs:\n",
    "            x.graphics[0].fgcolor = \"#b3b3b3\"\n",
    "            x.graphics[0].bgcolor = \"#ffffff\"\n",
    "            # x.graphics[0].width = 40\n",
    "            # x.graphics[0].height = 40\n",
    "\n",
    "        for x in pathway.genes:\n",
    "            x.graphics[0].fgcolor = \"#b3b3b3\"\n",
    "            x.graphics[0].bgcolor = \"#ffffff\"\n",
    "            # x.graphics[0].width = 40\n",
    "            # x.graphics[0].height = 40\n",
    "\n",
    "        for x in pathway.genes:\n",
    "            x.graphics[0].fgcolor = \"#b3b3b3\"\n",
    "            x.graphics[0].bgcolor = \"#ffffff\"\n",
    "            # x.graphics[0].width = 40\n",
    "            # x.graphics[0].height = 40\n",
    "\n",
    "        canvas = KGMLCanvas(pathway, import_imagemap=True)\n",
    "        kegg_pdf_path = \"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_kegg_map_\"+kegg_path+\".pdf\"\n",
    "        canvas.draw(kegg_pdf_path)\n",
    "        # PDF(\"fab_map_with_image.pdf\")\n",
    "\n",
    "        # Store Pdf with convert_from_path function\n",
    "        poppler = False\n",
    "        if poppler:\n",
    "            images = convert_from_path(kegg_pdf_path, poppler_path = r\"C:\\ProgramFilesFolder\\poppler-23.07.0\\Library\\bin\")\n",
    "            for i in range(len(images)):\n",
    "                images[i].save('page'+ str(i) +'.jpg', 'JPEG')\n",
    "            Image(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_kegg_map_\"+kegg_path+\".jpg\")\n",
    "        print(\"Created KEGG map\")\n",
    "    except Exception:\n",
    "        print(\"Problem Creating KEGG map \\n\\n Try disabling VPN if active \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa222e-9ca2-4207-bad2-bdd941ebf702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Complete\\n')\n",
    "input(\"Press enter to exit...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
