{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cfcfd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('__________________________')\n",
    "print('')\n",
    "print(\"Initialize Spectra Spectre\")\n",
    "print('__________________________')\n",
    "import os, sys, json, time, fnmatch, glob, warnings, math, subprocess\n",
    "from massql import msql_fileloading, msql_engine\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pyteomics import mzxml, mzml\n",
    "import numpy as np\n",
    "from scipy.integrate import trapz\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "starting_directory = os.getcwd()\n",
    "# pyinstaller command\n",
    "# pyinstaller --noconfirm --noupx -F --console --collect-all \"massql\" --collect-all \"matchms\" --collect-all \"pyarrow\" --collect-all \"pymzml\" --exclude-module \"kaleido\"  \"<absolute_path_to_script>\"\n",
    "\n",
    "# Convert jupyter notebok to script\n",
    "# jupyter nbconvert --to script \"<absolute_path_to_notebook>.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e5a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Do not run in Jupyter\"\"\"\n",
    "# if getattr(sys, 'frozen', False):\n",
    "#     bundle_dir = sys._MEIPASS\n",
    "# else:\n",
    "#     bundle_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# if getattr(sys, 'frozen', False) and hasattr(sys, '_MEIPASS'):\n",
    "#     print('Running in a PyInstaller bundle')\n",
    "# else:\n",
    "#     print('Running in a normal Python process')\n",
    "# print('')\n",
    "# print( 'bundle dir is', bundle_dir )\n",
    "# print( 'sys.argv[0] is', sys.argv[0] )\n",
    "# print( 'sys.executable is', sys.executable )\n",
    "# print( 'os.getcwd is', os.getcwd() )\n",
    "# print('__________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441662d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Configure\"\"\"\n",
    "try:\n",
    "    with open(\"spectre_config.json\") as config_file:\n",
    "        config = json.load(config_file)\n",
    "        use_queryfile = config['use_queryfile']\n",
    "        print(\"Use Query File Excel: \"+str(use_queryfile))\n",
    "        queryfile = config['queryfile']\n",
    "        print(\"Query File Excel: \"+str(queryfile))\n",
    "        use_queryfilejson = config['use_queryfile_json']\n",
    "        print(\"Use Query File JSON: \"+str(use_queryfilejson))\n",
    "        queryfilejson = config['queryfile_json']\n",
    "        print(\"Query File JSON: \"+str(queryfilejson))\n",
    "        metadata_settings = config['metadata']\n",
    "        print(metadata_settings)\n",
    "        cache_setting = config['cache']\n",
    "        print(\"Use Cache: \"+str(cache_setting))\n",
    "        datasaver = config['datasaver']\n",
    "        print(\"Use Datasaver: \"+str(datasaver))\n",
    "        data_directory = config['data_directory']\n",
    "        print(\"Data Directory: \"+str(data_directory))\n",
    "        QC_files = config['QC_files']\n",
    "        kegg_path = config['kegg_path']\n",
    "        convert_raw = config['convert_raw']\n",
    "        print(\"Convert raw: \"+str(convert_raw))\n",
    "        msconvertexe = config['msconvert_exe']\n",
    "        if convert_raw:\n",
    "            print(\"MSConvert exe: \"+str(msconvertexe))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"FileNotFoundError\\n\"\n",
    "          f\"{e} \\n\"\n",
    "          f\"Not found in \"+os.getcwd()+\"\\n\")\n",
    "    input(\"Press enter to exit...\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b38d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Definition Used to Generate a Query from Excel\"\"\"\n",
    "def create_query(name, KEGG, MS1_MZ, MS1_MZ_tolerance_ppm, rtmin, rtmax, group=\"A\"):\n",
    "    query = \"QUERY scaninfo(MS1DATA) FILTER MS1MZ=\"+     str(MS1_MZ)+\":TOLERANCEPPM=\"+     str(MS1_MZ_tolerance_ppm)+     \" AND RTMIN=\"+(str(rtmin))+     \" AND RTMAX=\"+str(rtmax)\n",
    "    return {'name':name, 'KEGG': KEGG, 'group':group, 'query':query, 'mslevel':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78188c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Create Queries from Query Files\"\"\"\n",
    "queries_excel = []\n",
    "query_groups = {}\n",
    "name_kegg_dict = {}\n",
    "MassQL_query_df = pd.DataFrame()\n",
    "if use_queryfile and queryfile:\n",
    "    try: \n",
    "        MassQL_query_df = pd.read_excel(queryfile)\n",
    "        print(\"\\nLoaded MassQL queries from: \"+str(queryfile)+\"\\n\")\n",
    "        for index, row in MassQL_query_df.iterrows():\n",
    "            if row['ion_mode'] == 1:\n",
    "                MS1MZ = row['Monoisotopic'] + 1.00725\n",
    "            else:\n",
    "                MS1MZ = row['Monoisotopic'] - 1.00725\n",
    "            queries_excel.append(create_query(row['Name'], row['KEGG'], MS1MZ, row['TOLERANCEPPM'], row['RTMIN'], row['RTMAX'], row['Group']))\n",
    "            name_kegg_dict.update({row['Name']: row['KEGG']})\n",
    "            query_groups.update({row['Name']: row['Group']})\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"FileNotFoundError\\n\"\n",
    "              f\"{e} \\n\"\n",
    "              f\"Not found in \"+os.getcwd()+\"\\n\")\n",
    "\n",
    "queries_json = []\n",
    "if use_queryfilejson:\n",
    "    if queryfilejson:\n",
    "        try:\n",
    "            with open(queryfilejson) as queryfilej:\n",
    "                queryjson = json.load(queryfilej)\n",
    "                print(\"\\nLoaded MassQL queries from: \"+str(queryfilejson)+\"\\n\")\n",
    "                for entry in queryjson:\n",
    "                    if \"scaninfo(MS2DATA)\" in entry['query']:\n",
    "                        mslevel = 2\n",
    "                    else:\n",
    "                        mslevel = 1\n",
    "                    entry.update({\"mslevel\": mslevel})\n",
    "                    queries_json.append(entry)\n",
    "                    name_kegg_dict.update({entry['name']: entry['KEGG']})\n",
    "                    query_groups.update({entry['name']: entry['group']})\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"FileNotFoundError\\n\"\n",
    "                  f\"{e} \\n\"\n",
    "                  f\"Not found in \"+os.getcwd()+\"\\n\")\n",
    "        \n",
    "queries = queries_excel + queries_json\n",
    "# display(queries)\n",
    "\n",
    "if queries:\n",
    "    print(\"\\nCreated \" + str(len(queries)) + \" MassQL Queries\")\n",
    "else:\n",
    "    print(\"No Queries Found\")\n",
    "    input(\"Press enter to exit...\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414ef76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Override MassQL definition to add datasaver function\"\"\"\n",
    "def custom_load_data_mzML_pyteomics(input_filename, datasaver=datasaver):\n",
    "    \"\"\"\n",
    "    This is a loading operation using pyteomics to help with loading mzML files with ion mobility\n",
    "    Args:\n",
    "        input_filename ([type]): [description]\n",
    "    \"\"\"\n",
    "\n",
    "    previous_ms1_scan = 0\n",
    "\n",
    "    # MS1\n",
    "    all_mz = []\n",
    "    all_rt = []\n",
    "    all_polarity = []\n",
    "    all_i = []\n",
    "    all_i_norm = []\n",
    "    all_i_tic_norm = []\n",
    "    all_scan = []\n",
    "\n",
    "    # MS2\n",
    "    all_msn_mz = []\n",
    "    all_msn_rt = []\n",
    "    all_msn_polarity = []\n",
    "    all_msn_i = []\n",
    "    all_msn_i_norm = []\n",
    "    all_msn_i_tic_norm = []\n",
    "    all_msn_scan = []\n",
    "    all_msn_precmz = []\n",
    "    all_msn_ms1scan = []\n",
    "    all_msn_charge = []\n",
    "    all_msn_mobility = []\n",
    "\n",
    "    with mzml.read(input_filename) as reader:\n",
    "        for spectrum in tqdm(reader):\n",
    "\n",
    "            if len(spectrum[\"intensity array\"]) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Getting the RT\n",
    "            try:\n",
    "                rt = spectrum[\"scanList\"][\"scan\"][0][\"scan start time\"]\n",
    "            except:\n",
    "                rt = 0\n",
    "            \n",
    "            # Correcting the unit\n",
    "            try:\n",
    "                if spectrum[\"scanList\"][\"scan\"][0][\"scan start time\"].unit_info == \"second\":\n",
    "                    rt = rt / 60\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            scan = int(spectrum[\"id\"].replace(\"scanId=\", \"\").split(\"scan=\")[-1])\n",
    "            \n",
    "            if not \"m/z array\" in spectrum:\n",
    "                # This is not a mass spectrum\n",
    "                continue\n",
    "\n",
    "            mzi_np = np.column_stack((spectrum[\"m/z array\"], spectrum[\"intensity array\"]))\n",
    "            if datasaver:\n",
    "                # if np.any(mzi_np > 0, axis=1):\n",
    "                mzi_np = np.delete(mzi_np, np.where(mzi_np[:, 1] == 0), axis=0)\n",
    "\n",
    "            mz, intensity = mzi_np.T\n",
    "            i_max = max(intensity)\n",
    "            i_sum = sum(intensity)\n",
    "\n",
    "            # If there is no ms level, its likely an UV/VIS spectrum and we can skip\n",
    "            if not \"ms level\" in spectrum:\n",
    "                continue\n",
    "            \n",
    "            mslevel = spectrum[\"ms level\"]\n",
    "            if mslevel == 1:\n",
    "                all_mz += list(mz)\n",
    "                all_i += list(intensity)\n",
    "                all_i_norm += list(intensity / i_max)\n",
    "                all_i_tic_norm += list(intensity / i_sum)\n",
    "                all_rt += len(mz) * [rt]\n",
    "                all_scan += len(mz) * [scan]\n",
    "                all_polarity += len(mz) * [msql_fileloading._determine_scan_polarity_pyteomics_mzML(spectrum)]\n",
    "\n",
    "                previous_ms1_scan = scan\n",
    "\n",
    "            if mslevel == 2:\n",
    "                msn_mz = spectrum[\"precursorList\"][\"precursor\"][0][\"selectedIonList\"][\"selectedIon\"][0][\"selected ion m/z\"]\n",
    "                msn_charge = 0\n",
    "\n",
    "                if \"charge state\" in spectrum[\"precursorList\"][\"precursor\"][0][\"selectedIonList\"][\"selectedIon\"][0]:\n",
    "                    msn_charge = int(spectrum[\"precursorList\"][\"precursor\"][0][\"selectedIonList\"][\"selectedIon\"][0][\"charge state\"])\n",
    "\n",
    "                all_msn_mz += list(mz)\n",
    "                all_msn_i += list(intensity)\n",
    "                all_msn_i_norm += list(intensity / i_max)\n",
    "                all_msn_i_tic_norm += list(intensity / i_sum)\n",
    "                all_msn_rt += len(mz) * [rt]\n",
    "                all_msn_scan += len(mz) * [scan]\n",
    "                all_msn_polarity += len(mz) * [msql_fileloading._determine_scan_polarity_pyteomics_mzML(spectrum)]\n",
    "                all_msn_precmz += len(mz) * [msn_mz]\n",
    "                all_msn_ms1scan += len(mz) * [previous_ms1_scan] \n",
    "                all_msn_charge += len(mz) * [msn_charge]\n",
    "\n",
    "                if \"product ion mobility\" in spectrum[\"precursorList\"][\"precursor\"][0][\"selectedIonList\"][\"selectedIon\"][0]:\n",
    "                    mobility = spectrum[\"precursorList\"][\"precursor\"][0][\"selectedIonList\"][\"selectedIon\"][0][\"product ion mobility\"]\n",
    "                    all_msn_mobility += len(mz) * [mobility]\n",
    "\n",
    "    ms1_df = pd.DataFrame()\n",
    "    if len(all_mz) > 0:\n",
    "        ms1_df['i'] = all_i\n",
    "        ms1_df['i_norm'] = all_i_norm\n",
    "        ms1_df['i_tic_norm'] = all_i_tic_norm\n",
    "        ms1_df['mz'] = all_mz\n",
    "        ms1_df['scan'] = all_scan\n",
    "        ms1_df['rt'] = all_rt\n",
    "        ms1_df['polarity'] = all_polarity\n",
    "        if datasaver:\n",
    "            for cat_col in ['scan', 'polarity']:\n",
    "                ms1_df = ms1_df[ms1_df[cat_col].notnull()].copy()\n",
    "                # ms1_df.loc[:, cat_col] = ms1_df[cat_col].astype('category')\n",
    "            # ms1_df['scan'] = ms1_df['scan'].astype('category')\n",
    "            # ms1_df['polarity'] = ms1_df['polarity'].astype('category')\n",
    "\n",
    "    ms2_df = pd.DataFrame()\n",
    "    if len(all_msn_mz) > 0:\n",
    "        ms2_df['i'] = all_msn_i\n",
    "        ms2_df['i_norm'] = all_msn_i_norm\n",
    "        ms2_df['i_tic_norm'] = all_msn_i_tic_norm\n",
    "        ms2_df['mz'] = all_msn_mz\n",
    "        ms2_df['scan'] = all_msn_scan\n",
    "        ms2_df['rt'] = all_msn_rt\n",
    "        ms2_df[\"polarity\"] = all_msn_polarity\n",
    "        ms2_df[\"precmz\"] = all_msn_precmz\n",
    "        ms2_df[\"ms1scan\"] = all_msn_ms1scan\n",
    "        ms2_df[\"charge\"] = all_msn_charge\n",
    "        if datasaver:\n",
    "            for cat_col in ['scan', 'polarity']:\n",
    "                ms2_df = ms2_df[ms2_df[cat_col].notnull()].copy()\n",
    "                # ms2_df.loc[:, cat_col] = ms2_df[cat_col].astype('category')\n",
    "        if len(all_msn_mobility) == len(all_msn_i):\n",
    "            ms2_df[\"mobility\"] = all_msn_mobility\n",
    "    \n",
    "    return ms1_df, ms2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e24bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"MassQL file loading\"\"\"\n",
    "def mq_load_data(input_filename, cache=False):\n",
    "    \"\"\"\n",
    "    Loading data generically\n",
    "    Args:\n",
    "        input_filename ([type]): [description]\n",
    "        cache (bool, optional): [description]. Defaults to False.\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    if cache:\n",
    "        ms1_filename = input_filename + \"_ms1.msql.feather\"\n",
    "        ms2_filename = input_filename + \"_ms2.msql.feather\"\n",
    "\n",
    "        # if ms1_filename in config_path or ms2_filename in config_path:\n",
    "\n",
    "        if os.path.exists(ms1_filename) or os.path.exists(ms2_filename):\n",
    "            try:\n",
    "                ms1_df = pd.read_feather(ms1_filename)\n",
    "            except:\n",
    "                ms1_df = pd.DataFrame()\n",
    "            try:\n",
    "                ms2_df = pd.read_feather(ms2_filename)\n",
    "            except:\n",
    "                ms2_df = pd.DataFrame()\n",
    "\n",
    "            return ms1_df, ms2_df\n",
    "\n",
    "    # Actually loading\n",
    "    if input_filename[-5:].lower() == \".mzml\":\n",
    "        #ms1_df, ms2_df = _load_data_mzML(input_filename)\n",
    "        #ms1_df, ms2_df = _load_data_mzML2(input_filename) # Faster version using pymzML\n",
    "        ms1_df, ms2_df = custom_load_data_mzML_pyteomics(input_filename) # Faster version using pymzML\n",
    "\n",
    "    elif input_filename[-6:].lower() == \".mzxml\":\n",
    "        ms1_df, ms2_df = msql_fileloading._load_data_mzXML(input_filename)\n",
    "    \n",
    "    elif input_filename[-5:] == \".json\":\n",
    "        ms1_df, ms2_df = msql_fileloading._load_data_gnps_json(input_filename)\n",
    "    \n",
    "    elif input_filename[-4:].lower() == \".mgf\":\n",
    "        ms1_df, ms2_df = msql_fileloading._load_data_mgf(input_filename)\n",
    "\n",
    "    elif input_filename[-4:].lower() == \".txt\" or input_filename[-4:].lower() == \".dat\":\n",
    "        ms1_df, ms2_df = msql_fileloading._load_data_txt(input_filename)\n",
    "    \n",
    "    else:\n",
    "        print(\"Cannot Load File Extension\")\n",
    "        raise Exception(\"File Format Not Supported\")\n",
    "\n",
    "\n",
    "    # Saving Cache\n",
    "    if cache:\n",
    "        ms1_filename = input_filename + \"_ms1.msql.feather\"\n",
    "        ms2_filename = input_filename + \"_ms2.msql.feather\"\n",
    "\n",
    "        if not (os.path.exists(ms1_filename) or os.path.exists(ms2_filename)):\n",
    "            try:\n",
    "                ms1_df.to_feather(ms1_filename)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                ms2_df.to_feather(ms2_filename)\n",
    "            except:\n",
    "                pass\n",
    "    return ms1_df, ms2_df\n",
    "\n",
    "print(\"\")\n",
    "os.chdir(data_directory)\n",
    "print('Current working directory is now data directory: '+os.getcwd())\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f562a02-860e-450e-89b4-4c666ec45152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Convert raw files\"\"\"\n",
    "path = os.getcwd()\n",
    "convert_count = 0\n",
    "if convert_raw and msconvertexe:\n",
    "    try:\n",
    "        subprocess.run(msconvertexe, \n",
    "                       stdout=subprocess.DEVNULL, \n",
    "                       stderr=subprocess.STDOUT, \n",
    "                       creationflags=subprocess.CREATE_NO_WINDOW)\n",
    "        for fn in os.listdir(path):\n",
    "            if \".raw\" in fn:\n",
    "                if os.path.isfile(path + '\\\\' + fn.replace('.raw','.mzML')):\n",
    "                    pass\n",
    "                    # print (path +\"\\\\\"+ fn + \" already exists\")\n",
    "                else:\n",
    "                    print (path + '\\\\' + fn + \" not converted yet!!\") \n",
    "                    subprocess.run(msconvertexe + \" \" + fn +  \" --zlib\", \n",
    "                                  stdout=subprocess.DEVNULL,\n",
    "                                   stderr=subprocess.STDOUT,\n",
    "                                   creationflags=subprocess.CREATE_NO_WINDOW)\n",
    "                    print(msconvertexe + \" \" + fn +  \" --zlib\")\n",
    "                    if os.path.isfile(path + '\\\\' + fn.replace('.raw','.mzML')):\n",
    "                        print (path + '\\\\' + fn + \" conversion completed!\")\n",
    "                        convert_count += 1\n",
    "                    else:\n",
    "                        print (path + '\\\\' + fn + \" conversion FAILED!\")\n",
    "    except Exception:\n",
    "        print(\"Path to MSConvert is Invalid. No raw files will be converted.\")\n",
    "else:\n",
    "    print(\"Not converting raw files (if any)\")\n",
    "\n",
    "if convert_count == 0:\n",
    "    print(\"No files converted\")\n",
    "elif convert_count > 0:\n",
    "    print(str(convert_count) + \" file(s) converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d2430-00cc-45d4-826f-49b4accfc38a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"metadata\"\"\"\n",
    "if metadata_settings[\"use_metadata\"]:\n",
    "    met_file = glob.glob('*'+str(metadata_settings[\"metadata_filename\"]))\n",
    "    if len(met_file) != 0:\n",
    "        metadata_df = pd.read_excel(met_file[0])\n",
    "        print(\"\\nLoaded metadata from: \"+str(met_file[0])+\"\\n\")\n",
    "    else:\n",
    "        metadata_df = pd.DataFrame()\n",
    "        print(\"\\nNo metadata file loaded\\n\")\n",
    "else:\n",
    "    metadata_df = pd.DataFrame()\n",
    "    print(\"\\nNo metadata file loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f93fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Query files\"\"\"\n",
    "import regex as re\n",
    "timestr = time.strftime(\"%Y_%m_%d_%H%M\")\n",
    "peak_area_df = pd.DataFrame()\n",
    "measured_rt_df = pd.DataFrame()\n",
    "raw_df = pd.DataFrame()\n",
    "all_results_list = []\n",
    "try:\n",
    "    file_count = len(fnmatch.filter(os.listdir(), '*.mzml'))\n",
    "    if file_count == 0:\n",
    "        # print('No mzml files found in '+os.getcwd()+\"\\\\DataMZML\\\\\\n\")\n",
    "        print('No mzml files found in '+os.getcwd()+\"\\\\\\n\")\n",
    "        input(\"Press enter to exit...\")\n",
    "        exit()\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"FileNotFoundError\\n\"\n",
    "          f\"{e} \\n\"\n",
    "          f\"Not found in \"+os.getcwd()+\"\\n\")\n",
    "    input(\"Press enter to exit...\")\n",
    "    exit()\n",
    "\n",
    "filename_groups = {}\n",
    "\n",
    "# Function to extract RTMIN value from a given input string\n",
    "def extract_rtmin_value(input_string):\n",
    "    pattern = r'RTMIN=(\\d+(?:\\.\\d+)?)'\n",
    "    match = re.search(pattern, input_string)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Function to extract RTMAX value from a given input string\n",
    "def extract_rtmax_value(input_string):\n",
    "    pattern = r'RTMAX=(\\d+(?:\\.\\d+)?)'\n",
    "    match = re.search(pattern, input_string)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return 99999\n",
    "\n",
    "def lookup_scan(row):\n",
    "    try:\n",
    "        collision_type = list(mzml_reader[row['scan']-1]['precursorList']['precursor'][0]['activation'].keys())[0];\n",
    "        energy = str(float(list(mzml_reader[row['scan']-1]['precursorList']['precursor'][0]['activation'].values())[1]));\n",
    "        collision_type_energy = str(collision_type) + \"__\"+ str(energy);\n",
    "        # collision_type_energy_list.append(collision_type_energy);\n",
    "    except:\n",
    "        collision_type = None\n",
    "        energy = None\n",
    "        collision_type_energy = None\n",
    "    return collision_type, energy, collision_type_energy\n",
    "    \n",
    "counter = 0\n",
    "filename_list = sorted(glob.iglob('*.mzML'))\n",
    "# filename_list = ['SysTest_v7_01_new.mzML']\n",
    "for filename in filename_list:\n",
    "   \n",
    "    filename_noext = os.path.splitext(filename)[0]\n",
    "    if not metadata_df.empty:\n",
    "        if not metadata_df.loc[metadata_df[metadata_settings[\"filename_column\"]] == filename_noext].empty:\n",
    "            filename_group = metadata_df.loc[metadata_df[metadata_settings[\"filename_column\"]] == filename_noext][metadata_settings[\"group_columns\"]].values[0][0]\n",
    "            filename_groups.update({filename_noext:filename_group})\n",
    "    counter += 1\n",
    "    print('')\n",
    "    print('----- Processing File '+str(counter)+' of '+str(file_count)+' -----')\n",
    "    ms1_df, ms2_df = mq_load_data(filename, cache=cache_setting)\n",
    "    for i, query in enumerate(queries):\n",
    "        \n",
    "        scannum_query = query['query'].replace(\"scaninfo\", \"scannum\")\n",
    "        results_df = pd.DataFrame()\n",
    "        try:\n",
    "            results_df = msql_engine.process_query(query['query'], filename, cache=cache_setting, ms1_df=ms1_df, ms2_df=ms2_df)\n",
    "            mzml_reader = mzml.MzML(filename)\n",
    "            rtmin_val = extract_rtmin_value(query['query'])\n",
    "            rtmax_val = extract_rtmax_value(query['query'])\n",
    "            results_df = results_df.loc[(results_df['rt'] > float(rtmin_val)) & (results_df.rt<float(rtmax_val))].copy()\n",
    "        except Exception:\n",
    "            results_df = pd.DataFrame()\n",
    "            print('Query failure\\n' + 'File: ' + str(filename) + '\\nQuery: ' + str(query['query']))\n",
    "            # print('Query will fail if running MS2 query on file without MS2 data')\n",
    "            pass \n",
    "\n",
    "        results_df['filename'] = filename\n",
    "        results_df['query_name'] = query['name']\n",
    "        results_df['query'] = query['query']\n",
    "\n",
    "        if query['mslevel'] == 2:\n",
    "            if not results_df.empty:\n",
    "                results_df[\"collision_type\"], results_df[\"energy\"], results_df[\"collision_type_energy\"] = zip(*results_df.apply(lambda row: lookup_scan(row), axis=1))\n",
    "\n",
    "        raw_df = pd.concat([raw_df, results_df], ignore_index=True)\n",
    "        \n",
    "        if query['mslevel'] == 2:\n",
    "            unique_name = filename + \"_ms2\"\n",
    "            group_name = \"ms2\"\n",
    "            if not results_df.empty:\n",
    "                for group_name, grouped_df in results_df.groupby('collision_type_energy'):\n",
    "                    unique_name = filename + \"_\" + group_name\n",
    "                    \n",
    "                    if not grouped_df.empty and len(grouped_df) > 0:\n",
    "    \n",
    "                        grouped_df = grouped_df.loc[grouped_df['i'].idxmax()].to_frame().T\n",
    "                        \n",
    "                        # peak_area = trapz(grouped_df.i, x=grouped_df.rt)\n",
    "                        peak_area = grouped_df.iloc[0][\"i\"]\n",
    "                        grouped_df['i'] = pd.to_numeric(grouped_df['i'])\n",
    "                        measured_RT = grouped_df.loc[grouped_df['i'].idxmax()]['rt']\n",
    "                        \n",
    "                        peak_area_df.at[unique_name, 'filename'] = filename\n",
    "                        peak_area_df.at[unique_name, 'file_directory'] = os.getcwd()\n",
    "                        peak_area_df.at[unique_name, query['name']] = peak_area\n",
    "                        peak_area_df.at[unique_name, 'collision_type_energy'] = group_name\n",
    "    \n",
    "                        measured_rt_df.at[unique_name, 'filename'] = filename\n",
    "                        measured_rt_df.at[unique_name, 'file_directory'] = os.getcwd()\n",
    "                        measured_rt_df.at[unique_name, query['name']] = measured_RT\n",
    "                        measured_rt_df.at[unique_name, 'collision_type_energy'] = group_name\n",
    "                        \n",
    "                        grouped_df.loc[:, \"query_name\"] = query['name']\n",
    "                        grouped_df.loc[:, \"group_name\"] = group_name\n",
    "                        grouped_df.loc[:, \"file\"] = os.getcwd()+\"\\\\\"+filename\n",
    "                        grouped_df.loc[:, \"file_directory\"] = os.getcwd()\n",
    "                        grouped_df.loc[:, \"filename\"] = filename\n",
    "                        grouped_df.loc[:, \"unique_name\"] = unique_name\n",
    "        \n",
    "                        all_results_list.append(grouped_df)\n",
    "                \n",
    "                    else:\n",
    "                        peak_area_df.at[unique_name, 'filename'] = filename\n",
    "                        peak_area_df.at[unique_name, 'file_directory'] = os.getcwd()\n",
    "                        peak_area_df.at[unique_name, query['name']] = 0\n",
    "                        peak_area_df.at[unique_name, 'collision_type_energy'] = group_name\n",
    "        \n",
    "                        measured_rt_df.at[unique_name, 'filename'] = filename\n",
    "                        measured_rt_df.at[unique_name, 'file_directory'] = os.getcwd()\n",
    "                        measured_rt_df.at[unique_name, query['name']] = 0\n",
    "                        measured_rt_df.at[unique_name, 'collision_type_energy'] = group_name\n",
    "                    \n",
    "        else:\n",
    "            unique_name = filename + \"_ms1\"\n",
    "            group_name = \"ms1\"\n",
    "            if not results_df.empty and len(results_df) >= 3:\n",
    "                peak_area = trapz(results_df.i, x=results_df.rt)\n",
    "                measured_RT = results_df.loc[results_df['i'].astype(float).idxmax()]['rt']\n",
    "\n",
    "                peak_area_df.at[unique_name, 'filename'] = filename\n",
    "                peak_area_df.at[unique_name, 'file_directory'] = os.getcwd()\n",
    "                peak_area_df.at[unique_name, query['name']] = peak_area\n",
    "                peak_area_df.at[unique_name, 'collision_type_energy'] = group_name\n",
    "                \n",
    "                measured_rt_df.at[unique_name, 'filename'] = filename\n",
    "                measured_rt_df.at[unique_name, 'file_directory'] = os.getcwd()\n",
    "                measured_rt_df.at[unique_name, query['name']] = measured_RT\n",
    "                measured_rt_df.at[unique_name, 'collision_type_energy'] = group_name\n",
    "                \n",
    "                results_df.loc[:, \"query_name\"] = query['name']\n",
    "                results_df.loc[:, \"group_name\"] = group_name\n",
    "                results_df.loc[:, \"file\"] = os.getcwd()+\"\\\\\"+filename\n",
    "                results_df.loc[:, \"file_directory\"] = os.getcwd()\n",
    "                results_df.loc[:, \"filename\"] = filename\n",
    "                results_df.loc[:, \"unique_name\"] = unique_name\n",
    "                results_df.at[:, 'collision_type_energy'] = group_name\n",
    "\n",
    "                all_results_list.append(results_df)\n",
    "\n",
    "            else:\n",
    "                peak_area_df.at[unique_name, 'filename'] = filename\n",
    "                peak_area_df.at[unique_name, 'file_directory'] = os.getcwd()\n",
    "                peak_area_df.at[unique_name, query['name']] = 0\n",
    "                peak_area_df.at[unique_name, 'collision_type_energy'] = \"ms1\"\n",
    "\n",
    "                measured_rt_df.at[unique_name, 'filename'] = filename\n",
    "                measured_rt_df.at[unique_name, 'file_directory'] = os.getcwd()\n",
    "                measured_rt_df.at[unique_name, query['name']] = 0\n",
    "                measured_rt_df.at[unique_name, 'collision_type_energy'] = \"ms1\"\n",
    "                \n",
    "\n",
    "if all_results_list:        \n",
    "    all_results_df = pd.concat(all_results_list)\n",
    "else:\n",
    "    all_results_df = pd.DataFrame()\n",
    "# print(results_df.memory_usage(index=True, deep=True).sum()/1000000000)\n",
    "\n",
    "if all_results_df.empty:\n",
    "    print('\\nNo matches for any query in any datafile\\n')\n",
    "    input(\"Press enter to exit...\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a7e751-6875-4e75-9c19-6788fe1debf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO create pdf of scans\n",
    "\n",
    "from pyteomics import pylab_aux as pa\n",
    "\n",
    "\"\"\"Save Results to Files\"\"\"\n",
    "\n",
    "def save_image(filename):\n",
    "    p = PdfPages(filename)\n",
    "    fig_nums = plt.get_fignums()\n",
    "    figs = [plt.figure(n) for n in fig_nums]\n",
    "    for fig in figs: \n",
    "        fig.savefig(p, format='pdf') \n",
    "    p.close()  \n",
    "\n",
    "if not os.path.exists(\"SpectraSpectre_Output/\"+timestr+\"/scans/\"):\n",
    "        os.makedirs(\"SpectraSpectre_Output/\"+timestr+\"/scans/\")\n",
    "    \n",
    "all_results_df_ms2 = all_results_df[all_results_df[\"mslevel\"] == 2]\n",
    "for group_name, group_df in all_results_df_ms2.groupby([\"filename\", \"query_name\"]):\n",
    "    \n",
    "    # display(group_df)\n",
    "    reader = mzml.MzML(group_name[0])\n",
    "    for index, row in group_df.iterrows():\n",
    "        plt.figure(figsize=(10,8))\n",
    "        # print(row['scan'], row['collision_type'], row['energy'])\n",
    "        spectrum = reader[int(row['scan'])-1]\n",
    "        spec_title = group_name[0], group_name[1], row['collision_type'], \"energy: \"+str(row['energy']), \"scan: \"+str(row['scan'])\n",
    "        spec_plot = pa.plot_spectrum(spectrum, title=spec_title)\n",
    "        # spec_plot = pa.plot_spectrum(spectrum, title=spec_title, backend='spectrum_utils')\n",
    "        # spec_plot = pa.plot_spectrum(spectrum, title=spec_title, precursor_mz=str(row['precmz']), precursor_charge=0, backend='spectrum_utils')\n",
    "\n",
    "        # plt.show()\n",
    "    # display(group_df)\n",
    "    # display(group_df[\"scan\"].tolist())\n",
    "    \n",
    "    \n",
    "    pdf_filename = \"SpectraSpectre_Output/\"+timestr+\"/scans/\"+group_name[0]+\"_\"+group_name[1]+\"_scans.pdf\"  \n",
    "    save_image(pdf_filename)\n",
    "    \n",
    "    plt.close(\"all\")\n",
    "    # break\n",
    "\n",
    "# fig_nums = plt.get_fignums()\n",
    "# display(fig_nums)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524ecd2-98bf-4d8f-9b30-3048a56b2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot Results\"\"\"\n",
    "for group_name, grouped_df in all_results_df.groupby(['query_name', 'collision_type_energy']):\n",
    "    fig1 = plt.figure(figsize=(12,8))\n",
    "    plt.subplots_adjust(bottom=0.3, top=.9, wspace = .1)\n",
    "    fig1.suptitle(group_name[0] + \" : \" + group_name[1], y = .96, fontsize=14)\n",
    "    fig1_sub1 = fig1.add_subplot(121, title='Intensity vs RT', xlabel='retention time', ylabel='intensity')\n",
    "    fig1_sub1.title.set_size(14)\n",
    "    fig1_sub1.set_ylabel('Intensity', fontsize=12)\n",
    "    fig1_sub1.set_xlabel('Retention Time', fontsize=12)\n",
    "    fig1_sub2 = fig1.add_subplot(122, title='peak area')\n",
    "    fig1_sub2.set_ylabel('peak area', fontsize=12)\n",
    "    fig1_sub2.title.set_size(14)\n",
    "    fig1_sub2.tick_params('x', labelrotation=90, labelsize=8)\n",
    "    fig1_sub2.yaxis.tick_right()\n",
    "    fig1_sub2.yaxis.set_label_position(\"right\")\n",
    "    fig1_sub2.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
    "    fig1_sub2.ticklabel_format(axis='x', style='sci', scilimits=(0, 0))\n",
    "    plt.subplots_adjust(bottom=0.3, top=.9, wspace = .1, left = 0.075)\n",
    "    for unique_name in grouped_df['unique_name'].unique():\n",
    "        filtered_data = grouped_df.loc[grouped_df['unique_name']==unique_name].copy()\n",
    "        filename = filtered_data['filename'].iloc[0]\n",
    "        peak_area = peak_area_df.loc[unique_name][group_name[0]]\n",
    "        fig1_sub1.scatter(filtered_data.rt, filtered_data.i)\n",
    "        fig1_sub1.plot(filtered_data.rt, filtered_data.i)\n",
    "        fig1_sub2.scatter(filename, peak_area)\n",
    "    ratio = 1.0\n",
    "    x_left, x_right = fig1_sub1.get_xlim()\n",
    "    y_low, y_high = fig1_sub1.get_ylim()\n",
    "    fig1_sub1.set_aspect(abs((x_right-x_left)/(y_low-y_high))*ratio)\n",
    "    x_left, x_right = fig1_sub2.get_xlim()\n",
    "    y_low, y_high = fig1_sub2.get_ylim()\n",
    "    fig1_sub2.set_aspect(abs((x_right-x_left)/(y_low-y_high))*ratio)\n",
    "\n",
    "\n",
    "if not os.path.exists(\"SpectraSpectre_Output/\"+timestr):\n",
    "    os.makedirs(\"SpectraSpectre_Output/\"+timestr)\n",
    "pdf_filename = \"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_images.pdf\"  \n",
    "save_image(pdf_filename)\n",
    "\n",
    "# peak_area_df_new = peak_area_df.reset_index(names=['CORE_Filename'])\n",
    "peak_area_df_new = peak_area_df.reset_index()\n",
    "peak_area_df_new = peak_area_df_new.rename(columns = {'index':'CORE_Filename'})\n",
    "\n",
    "measured_rt_df_new = measured_rt_df.reset_index()\n",
    "measured_rt_df_new = measured_rt_df_new.rename(columns = {'index':'CORE_Filename'})\n",
    "\n",
    "raw_df.to_csv(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_raw_query_df.csv\")  \n",
    "\n",
    "# with pd.ExcelWriter(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_results.xlsx\") as writer:\n",
    "#     peak_area_df_new.to_excel(writer, sheet_name=\"results\", index=False)\n",
    "#     if not MassQL_query_df.empty:\n",
    "#         MassQL_query_df.to_excel(writer, sheet_name=\"queries\", index=False)\n",
    "    \n",
    "peak_area_df_new = peak_area_df_new.drop(columns=['file_directory', 'collision_type_energy', 'filename'])\n",
    "peak_area_df_transpose = peak_area_df_new\n",
    "\n",
    "measured_rt_df_new = measured_rt_df_new.drop(columns=['file_directory', 'collision_type_energy', 'filename'])\n",
    "measured_rt_df_transpose = measured_rt_df_new\n",
    "\n",
    "def remove_filename_ext(filenameext):\n",
    "    # filenameext = str(filenameext)  # cast to string\n",
    "    # filenamenoext = filenameext[:-5] # remove last five characters\n",
    "    filenamenoext = os.path.splitext(str(filenameext))[0]\n",
    "    return str(filenamenoext)\n",
    "\n",
    "peak_area_df_transpose.set_index('CORE_Filename',inplace=True)\n",
    "peak_area_df_transpose = peak_area_df_transpose.T\n",
    "peak_area_df_transpose.to_csv(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_peak_areas.csv\")  \n",
    "\n",
    "measured_rt_df_transpose.set_index('CORE_Filename',inplace=True)\n",
    "measured_rt_df_transpose = measured_rt_df_transpose.T\n",
    "measured_rt_df_transpose.to_csv(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_retention_times.csv\")  \n",
    "\n",
    "print(\"\\nResults saved to:\")\n",
    "print(os.getcwd()+\"\\\\SpectraSpectre_Output\\n\")\n",
    "# print('Complete\\n')\n",
    "# input(\"Press enter to exit...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd4db5-283c-4b40-b7fb-b48f8daa1c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename_groups_dict = {}\n",
    "for key, value in filename_groups.items():\n",
    "   if value in filename_groups_dict:\n",
    "       filename_groups_dict[value].append(key)\n",
    "   else:\n",
    "       filename_groups_dict[value]=[key]\n",
    "\n",
    "query_groups_dict = {}\n",
    "for key, value in query_groups.items():\n",
    "   if value in query_groups_dict:\n",
    "       query_groups_dict[value].append(key)\n",
    "   else:\n",
    "       query_groups_dict[value]=[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee61c4-4411-4e30-b513-d402a5eb5963",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = peak_area_df_transpose.copy()\n",
    "analysis_df_T = peak_area_df_transpose.copy().T\n",
    "\n",
    "analysis_df_filename_grouped = pd.DataFrame()\n",
    "analysis_df_query_grouped = pd.DataFrame()\n",
    "analysis_df_both_grouped = pd.DataFrame()\n",
    "\n",
    "\n",
    "for fname_group, fnames in filename_groups_dict.items():\n",
    "    analysis_df_filename_grouped[fname_group] = analysis_df[fnames].sum(axis=1)\n",
    "\n",
    "for qname_group, qnames in query_groups_dict.items():\n",
    "    analysis_df_query_grouped[qname_group] = analysis_df_T[qnames].sum(axis=1)\n",
    "analysis_df_query_grouped_T = analysis_df_query_grouped.T\n",
    "analysis_df_query_grouped_T.columns.name = None\n",
    "\n",
    "for fname_group, fnames in filename_groups_dict.items():\n",
    "    analysis_df_both_grouped[fname_group] = analysis_df_query_grouped_T[fnames].sum(axis=1)\n",
    "\n",
    "dtype_s = analysis_df_query_grouped_T.select_dtypes(include='object').columns\n",
    "analysis_df_query_grouped_T[dtype_s] = analysis_df_query_grouped_T[dtype_s].astype(\"float\")\n",
    "\n",
    "analysis_df_filename_grouped_log2 = np.log2(1+analysis_df_filename_grouped)\n",
    "analysis_df_query_grouped_T_log2 = np.log2(1+analysis_df_query_grouped_T)\n",
    "analysis_df_both_grouped_log2 = np.log2(1+analysis_df_both_grouped)\n",
    "\n",
    "# display(analysis_df_filename_grouped)\n",
    "# display(1+analysis_df_filename_grouped)\n",
    "\n",
    "from  itertools import combinations\n",
    "\n",
    "analysis_df_filename_grouped_ratio = {f'{a}/{b}': analysis_df_filename_grouped[a].div(analysis_df_filename_grouped[b]) for a, b in combinations(analysis_df_filename_grouped.columns, 2)}\n",
    "if analysis_df_filename_grouped_ratio:\n",
    "    analysis_df_filename_grouped_ratio = pd.concat(analysis_df_filename_grouped_ratio, axis=1)\n",
    "    analysis_df_filename_grouped_ratio = analysis_df_filename_grouped_ratio.fillna(\"Null\")\n",
    "else:\n",
    "    analysis_df_filename_grouped_ratio = pd.DataFrame()\n",
    "\n",
    "analysis_df_query_grouped = analysis_df_query_grouped_T.T\n",
    "\n",
    "\n",
    "analysis_df_query_grouped_ratio = {f'{a}/{b}': analysis_df_query_grouped[a].div(analysis_df_query_grouped[b]) for a, b in combinations(analysis_df_query_grouped.columns, 2)}\n",
    "if analysis_df_query_grouped_ratio:\n",
    "    analysis_df_query_grouped_ratio = pd.concat(analysis_df_query_grouped_ratio, axis=1)\n",
    "    analysis_df_query_grouped_T_ratio = analysis_df_query_grouped_ratio.T\n",
    "    analysis_df_query_grouped_T_ratio = analysis_df_query_grouped_T_ratio.fillna(\"Null\")\n",
    "else:\n",
    "    analysis_df_query_grouped_T_ratio = pd.DataFrame()\n",
    "\n",
    "analysis_df_both_grouped_ratio = {f'{a}/{b}': analysis_df_both_grouped[a].div(analysis_df_both_grouped[b]) for a, b in combinations(analysis_df_both_grouped.columns, 2)}\n",
    "if analysis_df_both_grouped_ratio:\n",
    "    analysis_df_both_grouped_ratio = pd.concat(analysis_df_both_grouped_ratio, axis=1)\n",
    "    analysis_df_both_grouped_ratio = analysis_df_both_grouped_ratio.fillna(\"Null\")\n",
    "else:\n",
    "    analysis_df_both_grouped_ratio = pd.DataFrame()\n",
    "\n",
    "with pd.ExcelWriter(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_analysis.xlsx\") as writer:\n",
    "    analysis_df_filename_grouped.to_excel(writer, sheet_name=\"filename_grouped\", index=True)\n",
    "    analysis_df_query_grouped_T.to_excel(writer, sheet_name=\"query_grouped\", index=True)\n",
    "    analysis_df_both_grouped.to_excel(writer, sheet_name=\"both_grouped\", index=True)\n",
    "    analysis_df_filename_grouped_ratio.to_excel(writer, sheet_name=\"filename_grouped_ratio\", index=True)\n",
    "    analysis_df_query_grouped_T_ratio.to_excel(writer, sheet_name=\"query_grouped_ratio\", index=True)\n",
    "    analysis_df_both_grouped_ratio.to_excel(writer, sheet_name=\"both_grouped_ratio\", index=True)\n",
    "\n",
    "    \n",
    "analysis_df_filename_grouped_log2_ratio = {f'{a}_{b}': analysis_df_filename_grouped_log2[a].sub(analysis_df_filename_grouped_log2[b]) for a, b in combinations(analysis_df_filename_grouped_log2.columns, 2)}\n",
    "if analysis_df_filename_grouped_log2_ratio:\n",
    "    analysis_df_filename_grouped_log2_ratio = pd.concat(analysis_df_filename_grouped_log2_ratio, axis=1)\n",
    "    analysis_df_filename_grouped_log2_ratio = analysis_df_filename_grouped_log2_ratio.fillna(\"Null\")\n",
    "else:\n",
    "    analysis_df_filename_grouped_log2_ratio = pd.DataFrame()\n",
    "\n",
    "analysis_df_query_grouped_log2 = analysis_df_query_grouped_T_log2.T\n",
    "\n",
    "analysis_df_query_grouped_log2_ratio = {f'{a}_{b}': analysis_df_query_grouped_log2[a].sub(analysis_df_query_grouped_log2[b]) for a, b in combinations(analysis_df_query_grouped_log2.columns, 2)}\n",
    "if analysis_df_query_grouped_log2_ratio:\n",
    "    analysis_df_query_grouped_log2_ratio = pd.concat(analysis_df_query_grouped_log2_ratio, axis=1)\n",
    "    analysis_df_query_grouped_T_log2_ratio = analysis_df_query_grouped_log2_ratio.T\n",
    "    analysis_df_query_grouped_T_log2_ratio = analysis_df_query_grouped_T_log2_ratio.fillna(\"Null\")\n",
    "else:\n",
    "    analysis_df_query_grouped_T_log2_ratio = pd.DataFrame()\n",
    "    \n",
    "analysis_df_both_grouped_log2_ratio = {f'{a}_{b}': analysis_df_both_grouped_log2[a].sub(analysis_df_both_grouped_log2[b]) for a, b in combinations(analysis_df_both_grouped_log2.columns, 2)}\n",
    "if analysis_df_both_grouped_log2_ratio:\n",
    "    analysis_df_both_grouped_log2_ratio = pd.concat(analysis_df_both_grouped_log2_ratio, axis=1)\n",
    "    analysis_df_both_grouped_log2_ratio = analysis_df_both_grouped_log2_ratio.fillna(\"Null\")\n",
    "else:\n",
    "    analysis_df_both_grouped_log2_ratio = pd.DataFrame()\n",
    "\n",
    "# analysis_df_filename_grouped_log2 = np.log2(analysis_df_filename_grouped)\n",
    "# analysis_df_query_grouped_T_log2 = np.log2(analysis_df_query_grouped_T)\n",
    "# analysis_df_both_grouped_log2 = np.log2(analysis_df_both_grouped)\n",
    "\n",
    "with pd.ExcelWriter(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_analysis_log2.xlsx\") as writer:\n",
    "    analysis_df_filename_grouped_log2.to_excel(writer, sheet_name=\"filename_grouped\", index=True)\n",
    "    analysis_df_query_grouped_T_log2.to_excel(writer, sheet_name=\"query_grouped\", index=True)\n",
    "    analysis_df_both_grouped_log2.to_excel(writer, sheet_name=\"both_grouped\", index=True)\n",
    "    analysis_df_filename_grouped_log2_ratio.to_excel(writer, sheet_name=\"filename_grouped_ratio\", index=True)\n",
    "    analysis_df_query_grouped_T_log2_ratio.to_excel(writer, sheet_name=\"query_grouped_ratio\", index=True)\n",
    "    analysis_df_both_grouped_log2_ratio.to_excel(writer, sheet_name=\"both_grouped_ratio\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bc1287-b038-4eac-8509-f12d9e12696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"System Suitability\"\"\"\n",
    "qthreshold = 0.1\n",
    "peak_area_df_new_r = peak_area_df_new.reset_index()\n",
    "\n",
    "QC_df = peak_area_df_new_r[peak_area_df_new_r['CORE_Filename'].str.startswith(tuple(QC_files))]\n",
    "QC_df_T = QC_df.set_index(\"CORE_Filename\").T\n",
    "QC_df_T['CORE_Filename'] = QC_df_T.index\n",
    "\n",
    "peak_area_df_new_T = peak_area_df_new_r.T\n",
    "if QC_df.empty:\n",
    "    print(\"No System Suitability Check\\n\")\n",
    "else:\n",
    "    peak_area_df_QC1 = peak_area_df_new_r.copy()\n",
    "    qc_dict = {'CORE_Filename': 'QC_Average'}\n",
    "    for index, row in QC_df_T.iterrows():\n",
    "        qname = row['CORE_Filename']\n",
    "        qave = QC_df.loc[:, qname].mean()\n",
    "        qc_dict.update({qname:qave})\n",
    "\n",
    "    qc_row = pd.Series(qc_dict)\n",
    "    peak_area_df_QC1 = pd.concat([peak_area_df_QC1, qc_row.to_frame().T], ignore_index=True)\n",
    "    peak_area_df_QC1 = peak_area_df_QC1.set_index('CORE_Filename')\n",
    "    peak_area_df_QC2 = peak_area_df_QC1.copy()\n",
    "\n",
    "    # peak_area_df_QC2.loc[\"2022MCF0031_p_QCcurve_02_02.mzML\",\"propionyl carnitine-d3\"] = 1001\n",
    "\n",
    "    for index, row in QC_df_T.iterrows():\n",
    "        qname = row['CORE_Filename']\n",
    "        qave = QC_df.loc[:, qname].mean()\n",
    "        peak_area_df_QC2[qname] = peak_area_df_QC2[qname].apply(lambda x: 1 if qave==x else (None if qave == 0 else x/qave))\n",
    "\n",
    "    peak_area_df_QC3 = peak_area_df_QC2.copy()\n",
    "\n",
    "    for index, row in QC_df_T.iterrows():\n",
    "        qname = row['CORE_Filename']\n",
    "        peak_area_df_QC3[qname] = peak_area_df_QC3[qname].apply(lambda x: True if math.isclose(1,x,abs_tol=qthreshold) else False)\n",
    "\n",
    "    peak_area_df_QC = peak_area_df_QC3.copy()\n",
    "    peak_area_df_QC['System_Suitability'] = peak_area_df_QC.all(axis=1)\n",
    "    peak_area_df_QC = peak_area_df_QC[~(peak_area_df_QC.index.str.startswith(tuple(QC_files)))]\n",
    "    peak_area_df_QC = peak_area_df_QC.loc[:, peak_area_df_QC.columns.str.startswith('System_Suitability')]\n",
    "\n",
    "    peak_area_df_QC = peak_area_df_QC.reset_index()\n",
    "    peak_area_df_QC1 = peak_area_df_QC1.reset_index()\n",
    "    peak_area_df_QC2 = peak_area_df_QC2.reset_index()\n",
    "    peak_area_df_QC3 = peak_area_df_QC3.reset_index()\n",
    "\n",
    "    with pd.ExcelWriter(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_QC.xlsx\") as writer:\n",
    "        peak_area_df_QC.to_excel(writer, sheet_name=\"QC\", index=False)\n",
    "        peak_area_df_QC1.to_excel(writer, sheet_name=\"QC1\", index=False)\n",
    "        peak_area_df_QC2.to_excel(writer, sheet_name=\"QC2\", index=False)\n",
    "        peak_area_df_QC3.to_excel(writer, sheet_name=\"QC3\", index=False)\n",
    "\n",
    "    print('System Suitability Results:\\n')\n",
    "    # print(QC_df['CORE_Filename'].values.tolist())\n",
    "    print('QC Files: ' + str(QC_df['CORE_Filename'].values.tolist()))\n",
    "    print('\\n')\n",
    "    print(peak_area_df_QC)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5948ae3e-5bd9-47d2-bcbb-792d4e8e52f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if kegg_path and not analysis_df_filename_grouped_log2_ratio.empty:\n",
    "    print(\"\\nCreating KEGG Maps\\n\")\n",
    "    # os.chdir(starting_directory)\n",
    "          \n",
    "    import Bio\n",
    "    Bio.__version__\n",
    "    import ssl\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "    from Bio import SeqIO\n",
    "    from Bio.KEGG.REST import *\n",
    "    from Bio.KEGG.KGML import KGML_parser\n",
    "    from Bio.Graphics.KGML_vis import KGMLCanvas\n",
    "    from Bio.Graphics.ColorSpiral import ColorSpiral\n",
    "    from IPython.display import Image, HTML\n",
    "    import random\n",
    "    from pdf2image import convert_from_path\n",
    "    from IPython.display import Image \n",
    "\n",
    "    # A bit of code that will help us display the PDF output\n",
    "    def PDF(filename):\n",
    "        return HTML('<iframe src=%s width=700 height=350></iframe>' % filename)\n",
    "\n",
    "    # A bit of helper code to shorten long text\n",
    "    def head(text, lines=10):\n",
    "        \"\"\" Print the first lines lines of the passed text.\"\"\"\n",
    "        print('\\n'.join(text.split('\\n')[:lines] + ['[...]']))\n",
    "\n",
    "    def rgb_to_hex(rgb):\n",
    "        rgb = tuple([int(val) for val in rgb])\n",
    "        return '#' + ''.join([hex(val)[2:] for val in rgb]).upper()\n",
    "\n",
    "    \n",
    "    # peak_area_df_transform = peak_area_df_new.copy()\n",
    "    \n",
    "    peak_area_df_transform = analysis_df_filename_grouped_log2_ratio.T.copy()\n",
    "    # def transform_func(number):\n",
    "    #     return np.log2(1+number)\n",
    "\n",
    "    # executing the function\n",
    "    bad_column = [\"CORE_Filename\", \"file_directory\"]\n",
    "    other_cols = peak_area_df_transform.columns.difference(bad_column)\n",
    "    # peak_area_df_transform[other_cols] = peak_area_df_transform[other_cols].apply(transform_func)\n",
    "    # display(peak_area_df_transform)\n",
    "    # peak_area_df_transform[other_cols]= peak_area_df_transform[other_cols].div(peak_area_df_transform[other_cols].iloc[0])\n",
    "    # display(peak_area_df_transform)\n",
    "\n",
    "    # displaying the DataFrame\n",
    "    # display(peak_area_df_transform)\n",
    "    \n",
    "\n",
    "    for index, row in peak_area_df_transform[other_cols].iterrows():\n",
    "        shade_dict = {}\n",
    "        if index == 0:\n",
    "            pass\n",
    "        else:\n",
    "            for col_name in peak_area_df_transform[other_cols].columns:\n",
    "                # print(f\"{col_name}: {row[col_name]}\")\n",
    "                if col_name in name_kegg_dict.keys():\n",
    "                    cid = str(name_kegg_dict[col_name])\n",
    "                    cid = cid.replace(u'\\xa0', u'')\n",
    "                    \n",
    "                    if row[col_name] > 0:\n",
    "                        if row[col_name] >= 1:\n",
    "                            mer = '#%02x%02x%02x' % (0, 0, 255)\n",
    "                        else:\n",
    "                            color_val = int(255*(1-row[col_name]))\n",
    "                            mer = '#%02x%02x%02x' % (color_val, color_val, 255)\n",
    "                    elif row[col_name] < 0:\n",
    "                        if row[col_name] <= -1:\n",
    "                            mer = '#%02x%02x%02x' % (255, 0, 0)\n",
    "                        else:\n",
    "                            color_val = int(255*(1+row[col_name]))\n",
    "                            mer = '#%02x%02x%02x' % (255, color_val, color_val)\n",
    "                    elif row[col_name] == 0:\n",
    "                        mer = rgb_to_hex([255,255,255])\n",
    "                    else:\n",
    "                        mer = rgb_to_hex([155,155,155])\n",
    "\n",
    "                    shade_dict.update({str(cid):mer})\n",
    "\n",
    "                    # from reportlab.lib.colors import HexColor\n",
    "        try:\n",
    "            kegg_path = \"hsa00020\"\n",
    "            pathway = KGML_parser.read(kegg_get(kegg_path, \"kgml\"))\n",
    "            for x in pathway.compounds:\n",
    "                # print(str(shade_dict[x.graphics[0].name]))\n",
    "                # print(shade_dict)\n",
    "                try:\n",
    "                    x.graphics[0].bgcolor = str(shade_dict[x.graphics[0].name]) + str('CC')\n",
    "                    # x.graphics[0].bgcolor = str(shade_dict[x.graphics[0].name])\n",
    "\n",
    "                    # x.graphics[0].fgcolor = str(shade_dict[x.graphics[0].name])\n",
    "                    if shade_dict[x.graphics[0].name] == '#FFFFFF' or shade_dict[x.graphics[0].name] == \"#969696\":   \n",
    "                        x.graphics[0].fgcolor = \"#000000\" + str('66')\n",
    "                    else:\n",
    "                        x.graphics[0].fgcolor = str(shade_dict[x.graphics[0].name]) + str('CC')\n",
    "                except Exception:\n",
    "                    pass\n",
    "                    # print('no match')\n",
    "                x.graphics[0].type=\"circle\"\n",
    "                # x.graphics[0].fgcolor = \"#d1d1d1\"\n",
    "                x.graphics[0].width = 30\n",
    "                x.graphics[0].height = 17\n",
    "    \n",
    "            for x in pathway.orthologs:\n",
    "                x.graphics[0].fgcolor = \"#b3b3b3\"\n",
    "                x.graphics[0].bgcolor = \"#ffffff\"\n",
    "                # x.graphics[0].width = 40\n",
    "                # x.graphics[0].height = 40\n",
    "    \n",
    "            for x in pathway.genes:\n",
    "                x.graphics[0].fgcolor = \"#b3b3b3\"\n",
    "                x.graphics[0].bgcolor = \"#ffffff\"\n",
    "                # x.graphics[0].width = 40\n",
    "                # x.graphics[0].height = 40\n",
    "    \n",
    "            canvas = KGMLCanvas(pathway, import_imagemap=True)\n",
    "            kegg_pdf_path = \"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_kegg_map_\"+kegg_path+\"_\"+str(index)+\".pdf\"\n",
    "            # print(shade_dict)\n",
    "\n",
    "            canvas.draw(kegg_pdf_path)\n",
    "            print(index + \" created\")\n",
    "\n",
    "        #     # PDF(\"fab_map_with_image.pdf\")\n",
    "    \n",
    "        #     # Store Pdf with convert_from_path function\n",
    "        #     poppler = False\n",
    "        #     # if poppler:\n",
    "        #     #     images = convert_from_path(kegg_pdf_path, poppler_path = r\"C:\\ProgramFilesFolder\\poppler-23.07.0\\Library\\bin\")\n",
    "        #     #     for i in range(len(images)):\n",
    "        #     #         images[i].save('page'+ str(i) +'.jpg', 'JPEG')\n",
    "        #     #     Image(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_kegg_map_\"+kegg_path+\".jpg\")\n",
    "        #     # print(\"Created KEGG map\")\n",
    "        except Exception:\n",
    "            print(\"Problem Creating KEGG map \\n\\n Try Enabling VPN if NOT active \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa222e-9ca2-4207-bad2-bdd941ebf702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Complete\\n')\n",
    "input(\"Press enter to exit...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
