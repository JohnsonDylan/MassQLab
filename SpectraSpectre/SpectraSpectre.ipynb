{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cfcfd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('__________________________')\n",
    "print('')\n",
    "print(\"Initialize Spectra Spectre\")\n",
    "print('__________________________')\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import massql\n",
    "from massql import msql_fileloading, msql_engine\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pyteomics import mzxml, mzml\n",
    "import numpy as np\n",
    "import fnmatch\n",
    "import glob\n",
    "from scipy.integrate import trapz\n",
    "import warnings\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# pyinstaller command\n",
    "# pyinstaller --noconfirm -F --console --collect-all \"massql\" --collect-all \"matchms\" --collect-all \"pyarrow\" --collect-all \"pymzml\"  \"<absolute_path_to_script>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e5a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Do not run in Jupyter\"\"\"\n",
    "# if getattr(sys, 'frozen', False):\n",
    "#     bundle_dir = sys._MEIPASS\n",
    "# else:\n",
    "#     bundle_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# if getattr(sys, 'frozen', False) and hasattr(sys, '_MEIPASS'):\n",
    "#     print('Running in a PyInstaller bundle')\n",
    "# else:\n",
    "#     print('Running in a normal Python process')\n",
    "# print('')\n",
    "# print( 'bundle dir is', bundle_dir )\n",
    "# print( 'sys.argv[0] is', sys.argv[0] )\n",
    "# print( 'sys.executable is', sys.executable )\n",
    "# print( 'os.getcwd is', os.getcwd() )\n",
    "# print('__________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441662d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Configure\"\"\"\n",
    "try:\n",
    "    with open(\"spectre_config.json\") as config_file:\n",
    "        config = json.load(config_file)\n",
    "        cache_setting = config['cache']\n",
    "        print(\"Use Cache: \"+str(cache_setting))\n",
    "        queryfile = config['queryfile']\n",
    "        datasaver = config['datasaver']\n",
    "        print(\"Use Datasaver: \"+str(datasaver))\n",
    "        data_directory = config['data_directory']\n",
    "        QC_files = config['QC_files']\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"FileNotFoundError\\n\"\n",
    "          f\"{e} \\n\"\n",
    "          f\"Not found in \"+os.getcwd()+\"\\n\")\n",
    "    input(\"Press enter to exit...\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b38d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Definition Used to Generate a Query\"\"\"\n",
    "def create_query(name, KEGG, MS1_MZ, MS1_MZ_tolerance_ppm, retention_range, integration_range=None):\n",
    "    query = \"QUERY scaninfo(MS1DATA) FILTER MS1MZ=\"+     str(MS1_MZ)+\":TOLERANCEPPM=\"+     str(MS1_MZ_tolerance_ppm)+     \" AND RTMIN=\"+(str(retention_range[0]))+     \" AND RTMAX=\"+str(retention_range[1])\n",
    "    if integration_range is None:\n",
    "        integration_range = retention_range\n",
    "    return {'name':name, 'KEGG': KEGG, 'query':query, 'retention_range': retention_range, 'integration_range': integration_range}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78188c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Create Queries from Query File\"\"\"\n",
    "try: \n",
    "    MassQL_query_df = pd.read_excel(queryfile)\n",
    "    print(\"\\nLoaded MassQL queries from: \"+str(queryfile))\n",
    "    queries = []\n",
    "    for index, row in MassQL_query_df.iterrows():\n",
    "        if row['ion_mode'] == 1:\n",
    "            MS1MZ = row['M+H']\n",
    "        else:\n",
    "            MS1MZ = row['M-H']\n",
    "        if row['INTEGRATION_MIN'] != 'Null':\n",
    "            queries.append(create_query(row['Name'], row['KEGG'], MS1MZ, row['TOLERANCEPPM'], (row['RTMIN'], row['RTMAX']), (row['INTEGRATION_MIN'], row['INTEGRATION_MAX'])))\n",
    "        else:\n",
    "            queries.append(create_query(row['Name'], row['KEGG'], MS1MZ, row['TOLERANCEPPM'], (row['RTMIN'], row['RTMAX'])))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"FileNotFoundError\\n\"\n",
    "          f\"{e} \\n\"\n",
    "          f\"Not found in \"+os.getcwd()+\"\\n\")\n",
    "    input(\"Press enter to exit...\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414ef76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Override MassQL definition to add datasaver function\"\"\"\n",
    "def custom_load_data_mzML_pyteomics(input_filename, datasaver=datasaver):\n",
    "    \"\"\"\n",
    "    This is a loading operation using pyteomics to help with loading mzML files with ion mobility\n",
    "    Args:\n",
    "        input_filename ([type]): [description]\n",
    "    \"\"\"\n",
    "\n",
    "    previous_ms1_scan = 0\n",
    "\n",
    "    # MS1\n",
    "    all_mz = []\n",
    "    all_rt = []\n",
    "    all_polarity = []\n",
    "    all_i = []\n",
    "    all_i_norm = []\n",
    "    all_i_tic_norm = []\n",
    "    all_scan = []\n",
    "\n",
    "    # MS2\n",
    "    all_msn_mz = []\n",
    "    all_msn_rt = []\n",
    "    all_msn_polarity = []\n",
    "    all_msn_i = []\n",
    "    all_msn_i_norm = []\n",
    "    all_msn_i_tic_norm = []\n",
    "    all_msn_scan = []\n",
    "    all_msn_precmz = []\n",
    "    all_msn_ms1scan = []\n",
    "    all_msn_charge = []\n",
    "    all_msn_mobility = []\n",
    "\n",
    "    with mzml.read(input_filename) as reader:\n",
    "        for spectrum in tqdm(reader):\n",
    "\n",
    "            if len(spectrum[\"intensity array\"]) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Getting the RT\n",
    "            try:\n",
    "                rt = spectrum[\"scanList\"][\"scan\"][0][\"scan start time\"]\n",
    "            except:\n",
    "                rt = 0\n",
    "            \n",
    "            # Correcting the unit\n",
    "            try:\n",
    "                if spectrum[\"scanList\"][\"scan\"][0][\"scan start time\"].unit_info == \"second\":\n",
    "                    rt = rt / 60\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            scan = int(spectrum[\"id\"].replace(\"scanId=\", \"\").split(\"scan=\")[-1])\n",
    "            \n",
    "            if not \"m/z array\" in spectrum:\n",
    "                # This is not a mass spectrum\n",
    "                continue\n",
    "\n",
    "            mzi_np = np.column_stack((spectrum[\"m/z array\"], spectrum[\"intensity array\"]))\n",
    "            if datasaver:\n",
    "                # if np.any(mzi_np > 0, axis=1):\n",
    "                mzi_np = np.delete(mzi_np, np.where(mzi_np[:, 1] == 0), axis=0)\n",
    "\n",
    "            mz, intensity = mzi_np.T\n",
    "            i_max = max(intensity)\n",
    "            i_sum = sum(intensity)\n",
    "\n",
    "            # If there is no ms level, its likely an UV/VIS spectrum and we can skip\n",
    "            if not \"ms level\" in spectrum:\n",
    "                continue\n",
    "            \n",
    "            mslevel = spectrum[\"ms level\"]\n",
    "            if mslevel == 1:\n",
    "                all_mz += list(mz)\n",
    "                all_i += list(intensity)\n",
    "                all_i_norm += list(intensity / i_max)\n",
    "                all_i_tic_norm += list(intensity / i_sum)\n",
    "                all_rt += len(mz) * [rt]\n",
    "                all_scan += len(mz) * [scan]\n",
    "                all_polarity += len(mz) * [msql_fileloading._determine_scan_polarity_pyteomics_mzML(spectrum)]\n",
    "\n",
    "                previous_ms1_scan = scan\n",
    "\n",
    "            if mslevel == 2:\n",
    "                msn_mz = spectrum[\"precursorList\"][\"precursor\"][0][\"selectedIonList\"][\"selectedIon\"][0][\"selected ion m/z\"]\n",
    "                msn_charge = 0\n",
    "\n",
    "                if \"charge state\" in spectrum[\"precursorList\"][\"precursor\"][0][\"selectedIonList\"][\"selectedIon\"][0]:\n",
    "                    msn_charge = int(spectrum[\"precursorList\"][\"precursor\"][0][\"selectedIonList\"][\"selectedIon\"][0][\"charge state\"])\n",
    "\n",
    "                all_msn_mz += list(mz)\n",
    "                all_msn_i += list(intensity)\n",
    "                all_msn_i_norm += list(intensity / i_max)\n",
    "                all_msn_i_tic_norm += list(intensity / i_sum)\n",
    "                all_msn_rt += len(mz) * [rt]\n",
    "                all_msn_scan += len(mz) * [scan]\n",
    "                all_msn_polarity += len(mz) * [msql_fileloading._determine_scan_polarity_pyteomics_mzML(spectrum)]\n",
    "                all_msn_precmz += len(mz) * [msn_mz]\n",
    "                all_msn_ms1scan += len(mz) * [previous_ms1_scan] \n",
    "                all_msn_charge += len(mz) * [msn_charge]\n",
    "\n",
    "                if \"product ion mobility\" in spectrum[\"precursorList\"][\"precursor\"][0][\"selectedIonList\"][\"selectedIon\"][0]:\n",
    "                    mobility = spectrum[\"precursorList\"][\"precursor\"][0][\"selectedIonList\"][\"selectedIon\"][0][\"product ion mobility\"]\n",
    "                    all_msn_mobility += len(mz) * [mobility]\n",
    "\n",
    "    ms1_df = pd.DataFrame()\n",
    "    if len(all_mz) > 0:\n",
    "        ms1_df['i'] = all_i\n",
    "        ms1_df['i_norm'] = all_i_norm\n",
    "        ms1_df['i_tic_norm'] = all_i_tic_norm\n",
    "        ms1_df['mz'] = all_mz\n",
    "        ms1_df['scan'] = all_scan\n",
    "        ms1_df['rt'] = all_rt\n",
    "        ms1_df['polarity'] = all_polarity\n",
    "        if datasaver:\n",
    "            for cat_col in ['scan', 'polarity']:\n",
    "                ms1_df = ms1_df[ms1_df[cat_col].notnull()].copy()\n",
    "                # ms1_df.loc[:, cat_col] = ms1_df[cat_col].astype('category')\n",
    "            # ms1_df['scan'] = ms1_df['scan'].astype('category')\n",
    "            # ms1_df['polarity'] = ms1_df['polarity'].astype('category')\n",
    "\n",
    "    ms2_df = pd.DataFrame()\n",
    "    if len(all_msn_mz) > 0:\n",
    "        ms2_df['i'] = all_msn_i\n",
    "        ms2_df['i_norm'] = all_msn_i_norm\n",
    "        ms2_df['i_tic_norm'] = all_msn_i_tic_norm\n",
    "        ms2_df['mz'] = all_msn_mz\n",
    "        ms2_df['scan'] = all_msn_scan\n",
    "        ms2_df['rt'] = all_msn_rt\n",
    "        ms2_df[\"polarity\"] = all_msn_polarity\n",
    "        ms2_df[\"precmz\"] = all_msn_precmz\n",
    "        ms2_df[\"ms1scan\"] = all_msn_ms1scan\n",
    "        ms2_df[\"charge\"] = all_msn_charge\n",
    "        if datasaver:\n",
    "            for cat_col in ['scan', 'polarity']:\n",
    "                ms2_df = ms2_df[ms2_df[cat_col].notnull()].copy()\n",
    "                # ms2_df.loc[:, cat_col] = ms2_df[cat_col].astype('category')\n",
    "        if len(all_msn_mobility) == len(all_msn_i):\n",
    "            ms2_df[\"mobility\"] = all_msn_mobility\n",
    "    \n",
    "    return ms1_df, ms2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e24bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"MassQL file loading\"\"\"\n",
    "def mq_load_data(input_filename, cache=False):\n",
    "    \"\"\"\n",
    "    Loading data generically\n",
    "    Args:\n",
    "        input_filename ([type]): [description]\n",
    "        cache (bool, optional): [description]. Defaults to False.\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    if cache:\n",
    "        ms1_filename = input_filename + \"_ms1.msql.feather\"\n",
    "        ms2_filename = input_filename + \"_ms2.msql.feather\"\n",
    "\n",
    "        # if ms1_filename in config_path or ms2_filename in config_path:\n",
    "\n",
    "        if os.path.exists(ms1_filename) or os.path.exists(ms2_filename):\n",
    "            try:\n",
    "                ms1_df = pd.read_feather(ms1_filename)\n",
    "            except:\n",
    "                ms1_df = pd.DataFrame()\n",
    "            try:\n",
    "                ms2_df = pd.read_feather(ms2_filename)\n",
    "            except:\n",
    "                ms2_df = pd.DataFrame()\n",
    "\n",
    "            return ms1_df, ms2_df\n",
    "\n",
    "    # Actually loading\n",
    "    if input_filename[-5:].lower() == \".mzml\":\n",
    "        #ms1_df, ms2_df = _load_data_mzML(input_filename)\n",
    "        #ms1_df, ms2_df = _load_data_mzML2(input_filename) # Faster version using pymzML\n",
    "        ms1_df, ms2_df = custom_load_data_mzML_pyteomics(input_filename) # Faster version using pymzML\n",
    "\n",
    "    elif input_filename[-6:].lower() == \".mzxml\":\n",
    "        ms1_df, ms2_df = msql_fileloading._load_data_mzXML(input_filename)\n",
    "    \n",
    "    elif input_filename[-5:] == \".json\":\n",
    "        ms1_df, ms2_df = msql_fileloading._load_data_gnps_json(input_filename)\n",
    "    \n",
    "    elif input_filename[-4:].lower() == \".mgf\":\n",
    "        ms1_df, ms2_df = msql_fileloading._load_data_mgf(input_filename)\n",
    "\n",
    "    elif input_filename[-4:].lower() == \".txt\" or input_filename[-4:].lower() == \".dat\":\n",
    "        ms1_df, ms2_df = msql_fileloading._load_data_txt(input_filename)\n",
    "    \n",
    "    else:\n",
    "        print(\"Cannot Load File Extension\")\n",
    "        raise Exception(\"File Format Not Supported\")\n",
    "\n",
    "\n",
    "    # Saving Cache\n",
    "    if cache:\n",
    "        ms1_filename = input_filename + \"_ms1.msql.feather\"\n",
    "        ms2_filename = input_filename + \"_ms2.msql.feather\"\n",
    "\n",
    "        if not (os.path.exists(ms1_filename) or os.path.exists(ms2_filename)):\n",
    "            try:\n",
    "                ms1_df.to_feather(ms1_filename)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                ms2_df.to_feather(ms2_filename)\n",
    "            except:\n",
    "                pass\n",
    "    return ms1_df, ms2_df\n",
    "\n",
    "print(\"\")\n",
    "os.chdir(data_directory)\n",
    "print('Current working directory is now data directory: '+os.getcwd())\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f93fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Query files\"\"\"\n",
    "peak_area_df = pd.DataFrame()\n",
    "all_results_list = []\n",
    "try:\n",
    "    # file_count = len(fnmatch.filter(os.listdir(\"DataMZML\\\\\"), '*.mzml'))\n",
    "    file_count = len(fnmatch.filter(os.listdir(), '*.mzml'))\n",
    "\n",
    "    if file_count == 0:\n",
    "        # print('No mzml files found in '+os.getcwd()+\"\\\\DataMZML\\\\\\n\")\n",
    "        print('No mzml files found in '+os.getcwd()+\"\\\\\\n\")\n",
    "\n",
    "        input(\"Press enter to exit...\")\n",
    "        exit()\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"FileNotFoundError\\n\"\n",
    "          f\"{e} \\n\"\n",
    "          f\"Not found in \"+os.getcwd()+\"\\n\")\n",
    "    input(\"Press enter to exit...\")\n",
    "    exit()\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "    \n",
    "counter = 0\n",
    "for filepath in sorted(glob.iglob('*.mzML')):\n",
    "    counter += 1\n",
    "    print('')\n",
    "    print('----- Processing File '+str(counter)+' of '+str(file_count)+' -----')\n",
    "    filename = filepath\n",
    "    ms1_df, ms2_df = mq_load_data(filepath, cache=cache_setting)\n",
    "    for i, query in enumerate(queries):\n",
    "        int_range = float(query['integration_range'][1]) - float(query['integration_range'][0])\n",
    "\n",
    "        results_df = msql_engine.process_query(query['query'], filepath, cache=cache_setting, ms1_df=ms1_df, ms2_df=ms2_df)\n",
    "        if not results_df.empty:\n",
    "            results_df = results_df.loc[(results_df['rt'] > query['integration_range'][0]-(int_range/2)) & (results_df.rt<query['integration_range'][1]+(int_range/2))]\n",
    "\n",
    "            if len(results_df) > 1:\n",
    "                \n",
    "                results_df_i = results_df.loc[(results_df['rt'] > query['integration_range'][0]) & (results_df.rt<query['integration_range'][1])].copy()\n",
    "                peak_area = trapz(results_df_i.i, x=results_df_i.rt)\n",
    "                results_df_i = pd.DataFrame()\n",
    "                peak_area_df.at[filename, 'file_directory'] = os.getcwd()\n",
    "                peak_area_df.at[filename, query['name']] = peak_area\n",
    "                results_df.loc[:, \"query_name\"] = query['name']\n",
    "                results_df.loc[:, \"file\"] = os.getcwd()+\"\\\\\"+filepath\n",
    "                results_df.loc[:, \"file_directory\"] = os.getcwd()\n",
    "                results_df.loc[:, \"filename\"] = filename\n",
    "                if datasaver:\n",
    "                    for cat_col in ['mslevel', 'query_name', 'file', 'file_directory', 'filename']:\n",
    "                        results_df = results_df[results_df[cat_col].notnull()].copy()\n",
    "                        # results_df.loc[:, cat_col] = results_df[cat_col].astype('category')\n",
    "                all_results_list.append(results_df)\n",
    "            else:\n",
    "                peak_area_df.at[filename, 'file_directory'] = os.getcwd()\n",
    "                peak_area_df.at[filename, query['name']] = 0 \n",
    "        else:\n",
    "            peak_area_df.at[filename, 'file_directory'] = os.getcwd()\n",
    "            peak_area_df.at[filename, query['name']] = 0 \n",
    "        \n",
    "results_df = pd.concat(all_results_list)\n",
    "# print(results_df.memory_usage(index=True, deep=True).sum()/1000000000)\n",
    "\n",
    "if results_df.empty:\n",
    "    print('\\nNo matches for any query in any datafile\\n')\n",
    "    input(\"Press enter to exit...\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476c24b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Integrate and Plot Results\"\"\"\n",
    "for i, query in enumerate(queries):\n",
    "    int_range = query['integration_range'][1] - query['integration_range'][0]\n",
    "    fig1 = plt.figure(figsize=(12,8))\n",
    "    plt.subplots_adjust(bottom=0.3, top=.9, wspace = .1)\n",
    "    fig1.suptitle(query['name'], y = .96, fontsize=16)\n",
    "    fig1_sub1 = fig1.add_subplot(121, title='Intensity vs RT', xlabel='retention time', ylabel='intensity')\n",
    "    fig1_sub1.title.set_size(14)\n",
    "    fig1_sub1.axvline(x=query['retention_range'][0], color='b')\n",
    "    fig1_sub1.axvline(x=query['retention_range'][1], color='b')\n",
    "    fig1_sub1.axvline(x=query['integration_range'][0], color='r', linestyle='--')\n",
    "    fig1_sub1.axvline(x=query['integration_range'][1], color='r', linestyle='--')\n",
    "    fig1_sub1.set_xlim([query['integration_range'][0]-(int_range/2), query['integration_range'][1]+(int_range/2)])\n",
    "    fig1_sub1.set_ylabel('Intensity', fontsize=12)\n",
    "    fig1_sub1.set_xlabel('Retention Time', fontsize=12)\n",
    "    fig1_sub2 = fig1.add_subplot(122, title='peak area')\n",
    "    # fig1_sub2.set_xlabel('file', fontsize=12)\n",
    "    fig1_sub2.set_ylabel('peak area', fontsize=12)\n",
    "    fig1_sub2.title.set_size(14)\n",
    "    fig1_sub2.tick_params('x', labelrotation=90, labelsize=8)\n",
    "    fig1_sub2.yaxis.tick_right()\n",
    "    fig1_sub2.yaxis.set_label_position(\"right\")\n",
    "    fig1_sub2.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
    "    fig1_sub2.ticklabel_format(axis='x', style='sci', scilimits=(0, 0))\n",
    "    plt.subplots_adjust(bottom=0.3, top=.9, wspace = .1, left = 0.075)\n",
    "    for file_n in results_df['file'].unique():\n",
    "        file_directory, filename = file_n.rsplit('\\\\', 1)\n",
    "        filtered_data = results_df.loc[(results_df['query_name']==query['name']) & (results_df['file']==file_n)].copy()\n",
    "        peak_area = peak_area_df.loc[filename][query['name']]\n",
    "        fig1_sub1.plot(filtered_data.rt, filtered_data.i)\n",
    "        fig1_sub2.scatter(filename, peak_area)\n",
    "    ratio = 1.0\n",
    "    x_left, x_right = fig1_sub1.get_xlim()\n",
    "    y_low, y_high = fig1_sub1.get_ylim()\n",
    "    fig1_sub1.set_aspect(abs((x_right-x_left)/(y_low-y_high))*ratio)\n",
    "    x_left, x_right = fig1_sub2.get_xlim()\n",
    "    y_low, y_high = fig1_sub2.get_ylim()\n",
    "    fig1_sub2.set_aspect(abs((x_right-x_left)/(y_low-y_high))*ratio)\n",
    "\n",
    "\"\"\"Save Results to Files\"\"\"\n",
    "timestr = time.strftime(\"%Y_%m_%d_%H%M\")\n",
    "def save_image(filename):\n",
    "    p = PdfPages(filename)\n",
    "    fig_nums = plt.get_fignums()\n",
    "    figs = [plt.figure(n) for n in fig_nums]\n",
    "    for fig in figs: \n",
    "        fig.savefig(p, format='pdf') \n",
    "    p.close()  \n",
    "\n",
    "if not os.path.exists(\"SpectraSpectre_Output/\"+timestr):\n",
    "    os.makedirs(\"SpectraSpectre_Output/\"+timestr)\n",
    "pdf_filename = \"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_images.pdf\"  \n",
    "save_image(pdf_filename) \n",
    "\n",
    "peak_area_df_new = peak_area_df.reset_index(names=['CORE_Filename'])\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_results.xlsx\") as writer:\n",
    "    peak_area_df_new.to_excel(writer, sheet_name=\"results\", index=False)\n",
    "    MassQL_query_df.to_excel(writer, sheet_name=\"queries\", index=False)\n",
    "\n",
    "    \n",
    "    \n",
    "peak_area_df_biopan = peak_area_df_new.drop(columns=['file_directory'])\n",
    "\n",
    "def remove_filename_ext(filenameext):\n",
    "    filenameext = str(filenameext)  # cast to string\n",
    "    filenamenoext = filenameext[:-5] # remove last five characters\n",
    "    return str(filenamenoext)\n",
    "\n",
    "peak_area_df_biopan['CORE_Filename'] =peak_area_df_biopan['CORE_Filename'].apply(remove_filename_ext)\n",
    "\n",
    "peak_area_df_biopan.set_index('CORE_Filename',inplace=True)\n",
    "peak_area_df_biopan = peak_area_df_biopan.T\n",
    "peak_area_df_biopan.to_csv(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_results_biopan.csv\")  \n",
    "\n",
    "print(\"\\nResults saved to:\")\n",
    "print(os.getcwd()+\"\\\\SpectraSpectre_Output\\n\")\n",
    "# print('Complete\\n')\n",
    "# input(\"Press enter to exit...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e3703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QC_df = peak_area_df_new[peak_area_df_new['CORE_Filename'].str.startswith(tuple(QC_files)) | peak_area_df_new['CORE_Filename'].str.startswith('QC_')]\n",
    "# QC_df = peak_area_df_new[peak_area_df_new['CORE_Filename'].str.startswith('QC_')]\n",
    "if QC_df.empty:\n",
    "    print(\"No System Suitability Check\\n\")\n",
    "else:\n",
    "    peak_area_df_QC1 = peak_area_df_new.copy()\n",
    "    qc_dict = {'CORE_Filename': 'QC_Average'}\n",
    "\n",
    "    for index, row in MassQL_query_df.iterrows():\n",
    "        qname = row['Name']\n",
    "        qave = QC_df.loc[:, qname].mean()\n",
    "        qc_dict.update({qname:qave})\n",
    "\n",
    "    qc_row = pd.Series(qc_dict)\n",
    "    peak_area_df_QC1 = pd.concat([peak_area_df_QC1, qc_row.to_frame().T], ignore_index=True)\n",
    "    peak_area_df_QC1 = peak_area_df_QC1.set_index('CORE_Filename')\n",
    "    peak_area_df_QC1 = peak_area_df_QC1.drop('file_directory', axis=1)\n",
    "    peak_area_df_QC2 = peak_area_df_QC1.copy()\n",
    "\n",
    "    peak_area_df_QC2.loc[\"2022MCF0031_p_QCcurve_02_02.mzML\",\"propionyl carnitine-d3\"] = 1001\n",
    "\n",
    "    for index, row in MassQL_query_df.iterrows():\n",
    "        qname = row['Name']\n",
    "        qthreshold = row['threshold']\n",
    "        qave = QC_df.loc[:, qname].mean()\n",
    "        peak_area_df_QC2[qname] = peak_area_df_QC2[qname].apply(lambda x: 1 if qave==x else (None if qave == 0 else x/qave))\n",
    "\n",
    "    peak_area_df_QC3 = peak_area_df_QC2.copy()\n",
    "\n",
    "    for index, row in MassQL_query_df.iterrows():\n",
    "        qname = row['Name']\n",
    "        qthreshold = row['threshold']\n",
    "        peak_area_df_QC3[qname] = peak_area_df_QC3[qname].apply(lambda x: True if math.isclose(1,x,abs_tol=qthreshold) else False)\n",
    "\n",
    "    peak_area_df_QC = peak_area_df_QC3.copy()\n",
    "    peak_area_df_QC['System_Suitability'] = peak_area_df_QC.all(axis=1)\n",
    "    # peak_area_df_QC = peak_area_df_QC[~peak_area_df_QC.index.str.startswith('QC_')]\n",
    "    peak_area_df_QC = peak_area_df_QC[~(peak_area_df_QC.index.str.startswith(tuple(QC_files)) | peak_area_df_QC.index.str.startswith('QC_'))]\n",
    "    peak_area_df_QC = peak_area_df_QC.loc[:, peak_area_df_QC.columns.str.startswith('System_Suitability')]\n",
    "\n",
    "    peak_area_df_QC = peak_area_df_QC.reset_index()\n",
    "    peak_area_df_QC1 = peak_area_df_QC1.reset_index()\n",
    "    peak_area_df_QC2 = peak_area_df_QC2.reset_index()\n",
    "    peak_area_df_QC3 = peak_area_df_QC3.reset_index()\n",
    "\n",
    "    with pd.ExcelWriter(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_QC.xlsx\") as writer:\n",
    "        peak_area_df_QC.to_excel(writer, sheet_name=\"QC\", index=False)\n",
    "        peak_area_df_QC1.to_excel(writer, sheet_name=\"QC1\", index=False)\n",
    "        peak_area_df_QC2.to_excel(writer, sheet_name=\"QC2\", index=False)\n",
    "        peak_area_df_QC3.to_excel(writer, sheet_name=\"QC3\", index=False)\n",
    "\n",
    "    print('System Suitability Results:\\n')\n",
    "    # print(QC_df['CORE_Filename'].values.tolist())\n",
    "    print('QC Files: ' + str(QC_df['CORE_Filename'].values.tolist()))\n",
    "    print('\\n')\n",
    "    print(peak_area_df_QC)\n",
    "    print('\\n')\n",
    "\n",
    "print('Complete\\n')\n",
    "# input(\"Press enter to exit...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
