{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901e8b5-4258-4edc-8e95-ee69f1793db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.stdout.write(f\"--------------------------------\\n   Initialize SpectraSpectre   \\n--------------------------------\\n\")\n",
    "sys.stdout.flush()\n",
    "import io, os, json, time, fnmatch, glob, warnings, subprocess, pandas as pd, numpy as np, regex as re, contextlib, textwrap\n",
    "from pathlib import Path\n",
    "from pyteomics import mzxml, mzml, auxiliary\n",
    "from scipy.integrate import trapz\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "from scipy.interpolate import interp1d\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from reportlab.lib.pagesizes import letter, A4, landscape\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.lib import colors, utils\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.platypus import Frame, PageTemplate, BaseDocTemplate, Image, Table, Paragraph, NextPageTemplate, PageBreak\n",
    "from massql import msql_fileloading, msql_engine\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# pyinstaller command\n",
    "# pyinstaller --noconfirm --noupx -F --console --collect-all \"massql\" --collect-all \"matchms\" --collect-all \"pyarrow\" --collect-all \"pymzml\" --exclude-module \"kaleido\"  \"<absolute_path_to_script>\"\n",
    "\n",
    "# Convert jupyter notebok to script\n",
    "# jupyter nbconvert --to script \"<absolute_path_to_notebook>.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbadd33-d080-494c-b124-2814b1289589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file(filename):\n",
    "    base_filename = os.path.basename(filename)\n",
    "    try:\n",
    "        script_path = os.path.abspath(__file__)\n",
    "    except NameError:\n",
    "        script_path = os.getcwd()\n",
    "    script_dir = os.path.dirname(script_path)\n",
    "    script_dir_one_level_up = os.path.dirname(script_dir)\n",
    "    filename_path_one_level_up = os.path.join(script_dir_one_level_up, base_filename)\n",
    "    filename_path_current_wd = os.path.join(os.getcwd(), base_filename)\n",
    "    filename_path_script_dir = os.path.join(script_dir, base_filename)\n",
    "    if os.path.exists(filename_path_one_level_up):\n",
    "        return filename_path_one_level_up\n",
    "    elif os.path.exists(filename_path_current_wd):\n",
    "        return filename_path_current_wd\n",
    "    elif os.path.exists(filename_path_script_dir):\n",
    "        return filename_path_script_dir\n",
    "    else: \n",
    "        return None\n",
    "\n",
    "def find_dir(filedir):\n",
    "    try:\n",
    "        script_path = os.path.abspath(__file__)\n",
    "    except NameError:\n",
    "        script_path = os.getcwd()\n",
    "    script_dir = os.path.dirname(script_path)\n",
    "    script_dir_one_level_up = os.path.dirname(script_dir)\n",
    "    filedir_one_level_up = os.path.join(script_dir_one_level_up, filedir)\n",
    "    filedir_current_wd = os.path.join(os.getcwd(), filedir)\n",
    "    filedir_script_dir = os.path.join(script_dir, filedir)\n",
    "    if os.path.exists(filedir):\n",
    "        return filedir\n",
    "    elif os.path.exists(filedir_one_level_up):\n",
    "        return filedir_one_level_up\n",
    "    elif os.path.exists(filedir_current_wd):\n",
    "        return filedir_current_wd\n",
    "    elif os.path.exists(filedir_script_dir):\n",
    "        return filedir_script_dir\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441662d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Configure\"\"\"\n",
    "\n",
    "def configure_SpectraSpectre(config_file_path=\"spectre_config.json\"):\n",
    "    # Initialize config as an empty dictionary in case the file doesn't exist or is invalid\n",
    "    config = {}\n",
    "\n",
    "    config_file_path_loc = find_file(config_file_path)\n",
    "    if config_file_path_loc:\n",
    "        try:\n",
    "            with open(config_file_path_loc, 'r') as config_file:\n",
    "                config = json.load(config_file)\n",
    "                sys.stdout.write(f\"\\nLoaded config file: {os.path.normpath(config_file_path_loc)}\\n\")\n",
    "                sys.stdout.flush()\n",
    "        except json.JSONDecodeError:\n",
    "            sys.stdout.write(f\"\\nError: {config_file_path_loc} contains invalid JSON.\\n\")\n",
    "            sys.stdout.flush()\n",
    "    else:\n",
    "        sys.stdout.write(f\"\\nconfig_file: {config_file_path} could not be located.\\nLoading defaults...\\n\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # Extract configuration values with defaults if not found\n",
    "    data_directory = config.get('data_directory', 'data/')\n",
    "    queryfile = config.get('queryfile', 'MassQL_Queries.json')\n",
    "    metadata_file = config.get('metadata_file')\n",
    "    metadata_filename_column = config.get('metadata_filename_column', \"CORE_Filename\")\n",
    "    metadata_group_columns = config.get('metadata_group_columns')\n",
    "    kegg_path = config.get('kegg_path')\n",
    "    convert_raw = config.get('convert_raw', False)\n",
    "    msconvertexe = config.get('msconvert_exe')\n",
    "    cache_setting = config.get('cache', True)\n",
    "\n",
    "    # Check if data_directory and queryfile paths exist\n",
    "    data_directory_loc = find_dir(data_directory)\n",
    "    if data_directory_loc:\n",
    "        sys.stdout.write(f\"\\ndata_directory: {os.path.normpath(data_directory_loc)}\")\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        sys.stdout.write(f\"\\nWarning: data_directory '{os.path.normpath(data_directory)}' could not be located.\\n\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    queryfile_loc = find_file(queryfile)\n",
    "    if queryfile_loc:\n",
    "        sys.stdout.write(f\"\\nqueryfile: {os.path.normpath(queryfile_loc)}\")\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        sys.stdout.write(f\"\\nWarning: queryfile '{queryfile}' could not be located.\\n\")\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    return data_directory_loc, queryfile_loc, metadata_file, metadata_filename_column, metadata_group_columns, kegg_path, convert_raw, msconvertexe, cache_setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78188c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Create Queries from Query File\"\"\"\n",
    "\n",
    "# Function to extract RTMIN value from a given input string\n",
    "def extract_rtmin_value(input_string):\n",
    "    pattern = r'RTMIN=(\\d+(?:\\.\\d+)?)'\n",
    "    match = re.search(pattern, input_string)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Function to extract RTMAX value from a given input string\n",
    "def extract_rtmax_value(input_string):\n",
    "    pattern = r'RTMAX=(\\d+(?:\\.\\d+)?)'\n",
    "    match = re.search(pattern, input_string)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    else:\n",
    "        return 99999\n",
    "        \n",
    "def create_queries(queryfile, queries=None, query_groups=None, name_kegg_dict=None):\n",
    "    if name_kegg_dict is None:\n",
    "        name_kegg_dict = {}\n",
    "    if query_groups is None:\n",
    "        query_groups = {}\n",
    "    if queries is None:\n",
    "        queries = []\n",
    "\n",
    "    ms1_query_df = pd.DataFrame()\n",
    "    ms2_query_df = pd.DataFrame()\n",
    "\n",
    "    if queryfile:\n",
    "        try:\n",
    "            with open(queryfile) as queryfilej:\n",
    "                queryjson = json.load(queryfilej)\n",
    "                # Process each entry in the JSON file\n",
    "                for entry in queryjson:\n",
    "                    if entry.get('query') and entry.get('name'):\n",
    "                        # Determine MS level based on query content\n",
    "                        mslevel = 2 if \"scaninfo(MS2DATA)\" in entry['query'] else 1\n",
    "                        entry.update({\"mslevel\": mslevel})\n",
    "                        # Extract RT min and max values\n",
    "                        rtmin_val = extract_rtmin_value(entry['query'])\n",
    "                        rtmax_val = extract_rtmax_value(entry['query'])\n",
    "                        entry.update({'rtmin': rtmin_val, 'rtmax': rtmax_val})\n",
    "                        queries.append(entry)\n",
    "                        # Update dictionaries with KEGG and group info\n",
    "                        if entry.get('KEGG'):\n",
    "                            name_kegg_dict.update({entry['name']: entry.get('KEGG')})\n",
    "                        if entry.get('group'):\n",
    "                            query_groups.update({entry['name']: entry.get('group')})\n",
    "                    else:\n",
    "                        sys.stdout.write(f\"\\n\\nInvalid query:\\n {entry}\\n\")\n",
    "                        sys.stdout.flush()\n",
    "                        \n",
    "                # Convert queries to DataFrame for MS1 and MS2\n",
    "                query_df = pd.DataFrame(data=queries)\n",
    "                ms1_query_df = query_df[query_df['mslevel'] == 1]\n",
    "                ms2_query_df = query_df[query_df['mslevel'] == 2]\n",
    "                sys.stdout.write(f\"\\nCreated {str(len(queries))} MassQL Queries from {queryfile}\")\n",
    "                sys.stdout.flush()\n",
    "        except FileNotFoundError:\n",
    "            sys.stdout.write(f\"\\n\\nError: The file '{queryfile}' was not found.\")\n",
    "            sys.stdout.flush()\n",
    "        except json.JSONDecodeError:\n",
    "            sys.stdout.write(f\"\\n\\nError: The file '{queryfile}' contains invalid JSON.\")\n",
    "            sys.stdout.flush()\n",
    "    else:\n",
    "        sys.stdout.write(\"\\nNo queryfile specified\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    return queries, ms1_query_df, ms2_query_df, query_groups, name_kegg_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f562a02-860e-450e-89b4-4c666ec45152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Convert raw files\"\"\"\n",
    "\n",
    "def convert_raw_files(convert_raw, msconvertexe, data_directory, convert_count = 0):\n",
    "    if convert_raw and msconvertexe:\n",
    "        try:\n",
    "            subprocess.run(msconvertexe, \n",
    "                           stdout=subprocess.DEVNULL, \n",
    "                           stderr=subprocess.STDOUT, \n",
    "                           creationflags=subprocess.CREATE_NO_WINDOW)\n",
    "            for fn in os.listdir(data_directory):\n",
    "                if \".raw\" in fn:\n",
    "                    if os.path.isfile(data_directory + '\\\\' + fn.replace('.raw','.mzML')):\n",
    "                        pass\n",
    "                    else:\n",
    "                        sys.stdout.write(f\"\\n{data_directory} \\\\ {fn} converting\")\n",
    "                        sys.stdout.flush()\n",
    "                        subprocess.run(msconvertexe + \" \" + fn +  \" --zlib\", \n",
    "                                      stdout=subprocess.DEVNULL,\n",
    "                                       stderr=subprocess.STDOUT,\n",
    "                                       creationflags=subprocess.CREATE_NO_WINDOW)\n",
    "                        if os.path.isfile(data_directory + '\\\\' + fn.replace('.raw','.mzML')):\n",
    "                            sys.stdout.write(f\", conversion complete!\")\n",
    "                            sys.stdout.flush()\n",
    "                            convert_count += 1\n",
    "                        else:\n",
    "                            sys.stdout.write(f\", conversion FAILED!\")\n",
    "                            sys.stdout.flush()\n",
    "            if convert_count == 0:\n",
    "                sys.stdout.write(f\"\\nNo files converted\")\n",
    "                sys.stdout.flush()\n",
    "            elif convert_count > 0:\n",
    "                sys.stdout.write(f\"\\n{str(convert_count)} file(s) converted\")\n",
    "                sys.stdout.flush()\n",
    "        except Exception:\n",
    "            sys.stdout.write(f\"\\nError. No raw files will be converted. Check path to MSConvert executable.\")\n",
    "            sys.stdout.flush()\n",
    "    else:\n",
    "        sys.stdout.write(f\"\\nNot converting raw files (if any)\")\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1171363e-8c4c-4e69-b82f-8b102e2004f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Verify files are present\"\"\"\n",
    "\n",
    "def mzml_file_count(data_directory, file_count=0):\n",
    "    try:\n",
    "        file_count = len(fnmatch.filter(os.listdir(data_directory), '*.mzml'))\n",
    "        if file_count == 0:\n",
    "            sys.stdout.write(f\"\\n\\nWarning: No mzml files found in {data_directory}\")\n",
    "            sys.stdout.flush()\n",
    "            # input(\"Press enter to exit...\")\n",
    "            # exit()\n",
    "        else:\n",
    "            sys.stdout.write(f\"\\n{file_count} mzML files found in {data_directory}\\n\")\n",
    "            sys.stdout.flush()\n",
    "    except FileNotFoundError as e:\n",
    "        sys.stdout.write(f\"\\nFileNotFoundError\\n{e}\\nNot found in {data_directory}\")\n",
    "        sys.stdout.flush()\n",
    "        # input(\"Press enter to exit...\")\n",
    "        # exit()\n",
    "    return file_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f93fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Query files\"\"\"\n",
    "\n",
    "def query_files(data_directory, queries, scan_attributes = True, cache_setting=True):\n",
    "     #scan_attributes: True/False. Use True if multiple collision parameters per file. Kills performance.\n",
    "    timestr = time.strftime(\"%Y_%m_%d_%H%M\")\n",
    "\n",
    "    #Function for getting collision energy information from scan\n",
    "    def lookup_scan(row):\n",
    "        try:\n",
    "            collision_type = list(mzml_reader[row['scan']-1]['precursorList']['precursor'][0]['activation'].keys())[0]\n",
    "            energy = str(float(list(mzml_reader[row['scan']-1]['precursorList']['precursor'][0]['activation'].values())[1]))\n",
    "        except:\n",
    "            collision_type = energy = \"None\"\n",
    "        return collision_type, energy\n",
    "    \n",
    "    raw_df_ms1 = pd.DataFrame()\n",
    "    raw_df_ms2 = pd.DataFrame()\n",
    "    raw_ms1_df_list = []\n",
    "    raw_ms2_df_list = []\n",
    "    filename_groups = {}\n",
    "    \n",
    "    stdout_buffer = io.StringIO()\n",
    "    stderr_buffer = io.StringIO()\n",
    "    counter = 0\n",
    "    fail_log = []\n",
    "\n",
    "    filename_list = sorted(glob.iglob(os.path.join(data_directory, '*.mzML')))\n",
    "    filename_list = sorted([os.path.normpath(path) for path in filename_list])\n",
    "    # filename_list = sorted([path.replace('\\\\', '/') for path in filename_list])\n",
    "\n",
    "    if queries:\n",
    "        # filename_list = filename_list[::-1]\n",
    "        # filename_list = filename_list[0:4] #subset files\n",
    "        for filename in filename_list:\n",
    "            filename_base = os.path.basename(filename)\n",
    "            fail_count = 0\n",
    "            counter += 1\n",
    "            sys.stdout.write(f\"\\nApplying 1/{len(queries)} MassQL Queries to File {counter} of {len(filename_list)}\")\n",
    "            sys.stdout.flush()\n",
    "            ms1_df, ms2_df = msql_fileloading.load_data(filename, cache=cache_setting)\n",
    "            if scan_attributes: \n",
    "                mzml_reader = mzml.MzML(filename)\n",
    "            for i, query in enumerate(queries):\n",
    "                i = i + 1\n",
    "                sys.stdout.write(f\"\\rApplying {i}/{len(queries)} MassQL Queries to File {counter} of {len(filename_list)} ({fail_count} Failed)\")\n",
    "                sys.stdout.flush()\n",
    "        \n",
    "                # scannum_query = query['query'].replace(\"scaninfo\", \"scannum\")\n",
    "                results_df = pd.DataFrame()\n",
    "                try:\n",
    "                    with contextlib.redirect_stdout(stdout_buffer), contextlib.redirect_stderr(stderr_buffer):\n",
    "                        results_df = msql_engine.process_query(query['query'], filename, cache=cache_setting, ms1_df=ms1_df, ms2_df=ms2_df)\n",
    "                    if not results_df.empty:\n",
    "                        results_df['filename'] = filename_base\n",
    "                        results_df['query_name'] = query['name']\n",
    "                        results_df['query'] = query['query']\n",
    "        \n",
    "                    # else:\n",
    "                    #     results_df_dict = {'filename': {0: filename}, \n",
    "                    #                        'query_name': {0: query['name']}, \n",
    "                    #                        'query': {0: query['query']},\n",
    "                    #                        'mslevel': {0: query['mslevel']}}\n",
    "                    #     results_df = pd.DataFrame(data=results_df_dict)\n",
    "                        \n",
    "                        if scan_attributes:\n",
    "                            if query['mslevel'] == 2:\n",
    "                                results_df[\"collision_type\"], results_df[\"energy\"] = zip(*results_df.apply(lambda row: lookup_scan(row), axis=1))\n",
    "                            else:\n",
    "                                results_df[\"collision_type\"], results_df[\"energy\"] = 'NULL', 'NULL'\n",
    "                    \n",
    "                except Exception:\n",
    "                    fail_count += 1\n",
    "                    fail_log.append(str(query['name'] + \" for \" +str(filename_base)))\n",
    "                    results_df = pd.DataFrame()\n",
    "                    sys.stdout.write(f\"\\rApplying {i}/{len(queries)} MassQL Queries to File {counter} of {len(filename_list)} ({fail_count} Failed)\")  # \\r moves the cursor to the beginning of the line\n",
    "                    sys.stdout.flush()\n",
    "                    # print(str(\"Query Failed: \" + query['name']) +' for ' + str(filename))\n",
    "                    pass \n",
    "        \n",
    "                if query['mslevel'] == 1:\n",
    "                    raw_ms1_df_list.append(results_df)\n",
    "                if query['mslevel'] == 2:\n",
    "                    raw_ms2_df_list.append(results_df)\n",
    "        \n",
    "        if fail_log:\n",
    "            fail_log = '\\n'.join(fail_log)\n",
    "            sys.stdout.write(f\"\\n\\nQueries Failed:\\n{fail_log}\\n\") \n",
    "            sys.stdout.flush()   \n",
    "        \n",
    "        results_df = pd.DataFrame()\n",
    "        if raw_ms1_df_list:\n",
    "            if not os.path.exists(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/scans/\"):\n",
    "                os.makedirs(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/scans/\")\n",
    "            raw_df_ms1 = pd.concat(raw_ms1_df_list)\n",
    "            \n",
    "        if raw_ms2_df_list:\n",
    "            if not os.path.exists(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/scans/\"):\n",
    "                os.makedirs(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/scans/\")\n",
    "            raw_df_ms2 = pd.concat(raw_ms2_df_list)\n",
    "            raw_df_ms2['collision_type'] = raw_df_ms2['collision_type'].apply(lambda x: 'CID' if x == 'collision-induced dissociation' else x)\n",
    "            raw_df_ms2['collision_type'] = raw_df_ms2['collision_type'].apply(lambda x: 'HCD' if x == 'beam-type collision-induced dissociation' else x)\n",
    "            raw_df_ms2['collision_type_energy'] = raw_df_ms2['collision_type'].astype(str) + \"__\" + raw_df_ms2['energy'].astype(str)\n",
    "            \n",
    "        if not raw_df_ms1.empty:\n",
    "            raw_df_ms1.to_csv(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_raw_df.csv\")\n",
    "            sys.stdout.write(f\"\\nCreated raw_df_ms1 and exported as csv.\") \n",
    "            sys.stdout.flush()\n",
    "        if not raw_df_ms2.empty:\n",
    "            raw_df_ms2.to_csv(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_raw_df.csv\")\n",
    "            sys.stdout.write(f\"\\nCreated raw_df_ms2 and exported as csv.\") \n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return raw_df_ms1, raw_df_ms2, filename_groups, timestr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b9806-b58e-49df-b63d-4d6e4da7abfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Analysis MS1\"\"\"\n",
    "\n",
    "# Analyzes raw MS1 dataframe by performing fit of peak and measuring peak parameters. \n",
    "# Additionally creates a new raw MS1 dataframe that contains the fit datapoints\n",
    "\n",
    "def analysis_ms1(raw_df_ms1, data_directory, timestr):\n",
    "\n",
    "    def index_to_xdata(xdata, indices):\n",
    "        \"interpolate the values from signal.peak_widths to xdata\"\n",
    "        ind = np.arange(len(xdata))\n",
    "        f = interp1d(ind,xdata)\n",
    "        return f(indices)\n",
    "\n",
    "    def custom_score(peak):\n",
    "        normalized_height = ydata_all[peak] / max(ydata_all)  # Normalize height by the maximum height in the signal\n",
    "        height_score = normalized_height  \n",
    "        closeness_score = 1 / ((abs(xdata_all[peak] - avg_rt))**(0.5))\n",
    "        # score = height_score * closeness_score\n",
    "        score = height_score\n",
    "        return score\n",
    "    \n",
    "    ms1_analysis_df = pd.DataFrame()\n",
    "    ms1_analysis_df_list = []\n",
    "    if not raw_df_ms1.empty:\n",
    "        with PdfPages(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_traces.pdf\") as pdf:\n",
    "            for group_name, grouped_df in raw_df_ms1.groupby(['filename', 'query_name', 'query']):\n",
    "                analysis_df = pd.DataFrame()    \n",
    "                if len(grouped_df) >= 5:\n",
    "                    rtmin = extract_rtmin_value(group_name[2])\n",
    "                    rtmax = extract_rtmax_value(group_name[2])\n",
    "                    avg_rt = (rtmin+rtmax)/2\n",
    "                    range_rt = rtmax-rtmin\n",
    "                    ydata_all = np.array(grouped_df.i)\n",
    "                    xdata_all = np.array(grouped_df.rt)\n",
    "\n",
    "                    if (np.max(ydata_all)/5) >= 50000:\n",
    "                        height_min = (np.max(ydata_all)/5)\n",
    "                    else:\n",
    "                        height_min = 50000\n",
    "        \n",
    "                    peaks, _ = find_peaks(ydata_all, height=height_min, distance=(5), prominence=height_min)\n",
    "                    sorted_peaks_prominence = sorted(peaks, key=lambda x: ydata_all[x], reverse=True)\n",
    "                    top_n_peaks_prominence = sorted_peaks_prominence[:2]\n",
    "                        \n",
    "                    # Sort peaks based on the custom scoring function\n",
    "                    top_n_peaks = sorted(top_n_peaks_prominence, key=custom_score, reverse=True)\n",
    "                    \n",
    "                    peak_x_values = xdata_all[top_n_peaks]\n",
    "                    peak_y_values = ydata_all[top_n_peaks]\n",
    "        \n",
    "                    # fwhm\n",
    "                    Awidths, Awidth_heights, Aleft_ips, Aright_ips = peak_widths(ydata_all, top_n_peaks, rel_height=0.5)\n",
    "                    Aleft_ips = index_to_xdata(xdata_all, Aleft_ips)\n",
    "                    Aright_ips = index_to_xdata(xdata_all, Aright_ips)\n",
    "                    Areal_width = Aright_ips - Aleft_ips\n",
    "                    plt.hlines(Awidth_heights, Aleft_ips, Aright_ips, color='r')\n",
    "        \n",
    "                    # fw 25% max\n",
    "                    Bwidths, Bwidth_heights, Bleft_ips, Bright_ips = peak_widths(ydata_all, top_n_peaks, rel_height=0.75)\n",
    "                    Bleft_ips = index_to_xdata(xdata_all, Bleft_ips)\n",
    "                    Bright_ips = index_to_xdata(xdata_all, Bright_ips)\n",
    "                    Breal_width = Bright_ips - Bleft_ips\n",
    "                    plt.hlines(Bwidth_heights, Bleft_ips, Bright_ips, color='r')\n",
    "        \n",
    "                    # fw 10% max\n",
    "                    Cwidths, Cwidth_heights, Cleft_ips, Cright_ips = peak_widths(ydata_all, top_n_peaks, rel_height=0.9)\n",
    "                    Cleft_ips = index_to_xdata(xdata_all, Cleft_ips)\n",
    "                    Cright_ips = index_to_xdata(xdata_all, Cright_ips)\n",
    "                    Creal_width = Cright_ips - Cleft_ips\n",
    "                    plt.hlines(Cwidth_heights, Cleft_ips, Cright_ips, color='r')\n",
    "        \n",
    "        \n",
    "                    if Areal_width.size != 0:\n",
    "                        linear_interp = interp1d(xdata_all, ydata_all)\n",
    "            \n",
    "                        interp_indicesA = np.concatenate((Aleft_ips, Aright_ips), axis=0)\n",
    "                        interp_indicesB = np.concatenate((Bleft_ips, Bright_ips), axis=0)\n",
    "                        interp_indicesC = np.concatenate((Cleft_ips, Cright_ips), axis=0)\n",
    "            \n",
    "                        x_interpA = np.linspace(min(interp_indicesA), max(interp_indicesA), 50)\n",
    "                        x_interpB = np.linspace(min(interp_indicesB), max(interp_indicesB), 50)\n",
    "                        x_interpC = np.linspace(min(interp_indicesC), max(interp_indicesC), 50)\n",
    "            \n",
    "                        x_interp = np.concatenate([x_interpA, x_interpB, x_interpC, xdata_all])\n",
    "                        x_interp = np.sort(x_interp)\n",
    "                        y_interp = linear_interp(x_interp)\n",
    "            \n",
    "                        plt.plot(x_interp, y_interp, '-b', label='Data2')\n",
    "        \n",
    "                    plt.plot(xdata_all, ydata_all, '-bo', label='Data')\n",
    "    \n",
    "                    plot_symbols = ['x', '+', '+', '+', '+']\n",
    "                    for peak_index, peak_num in enumerate(top_n_peaks):\n",
    "                        plt.plot(xdata_all[peak_num], ydata_all[peak_num], str(plot_symbols[peak_index]), color='r')\n",
    "    \n",
    "                    if Areal_width.size != 0:\n",
    "                        if (2*(Areal_width[0])) < Breal_width[0]:\n",
    "                            int_min, int_max = Aleft_ips[0], Aright_ips[0]\n",
    "                        elif (2*(Breal_width[0])) < Creal_width[0]:\n",
    "                            int_min, int_max = Bleft_ips[0], Bright_ips[0]\n",
    "                        else:\n",
    "                            int_min, int_max = Cleft_ips[0], Cright_ips[0]\n",
    "                    else:\n",
    "                        int_min = int_max = None\n",
    "                    \n",
    "                    plt.xlim(xmin=(rtmin-range_rt), xmax=(rtmax+range_rt))\n",
    "                    plt.axvline(x=rtmin, color='black', linestyle=':')\n",
    "                    plt.axvline(x=rtmax, color='black', linestyle=':')\n",
    "        \n",
    "                    if int_min:\n",
    "                        plt.axvline(x=int_min, color='green', linestyle='--')\n",
    "                        plt.axvline(x=int_max, color='green', linestyle='--')\n",
    "                    \n",
    "                    plt.xlabel('rt')\n",
    "                    plt.ylabel('intensity')\n",
    "                    plt.title(str(group_name[0]) + '\\n' + str(group_name[1]))\n",
    "                    pdf.savefig()\n",
    "                    # plt.show()\n",
    "                    plt.close()\n",
    "    \n",
    "                    peak_area_alt = trapz(grouped_df.i, x=grouped_df.rt)      \n",
    "                    if int_min:\n",
    "                        mask = (x_interp >= int_min) & (x_interp <= int_max)\n",
    "                        x_subset = x_interp[mask]\n",
    "                        y_subset = y_interp[mask]\n",
    "                        peak_area = trapz(y_subset, x=x_subset)\n",
    "        \n",
    "                        measured_RT = peak_x_values[0]\n",
    "                        fwhm = Areal_width[0]\n",
    "                        rt_error = 0 if rtmin <= peak_x_values[0] <= rtmax else peak_x_values[0] - rtmin if peak_x_values[0] < rtmin else peak_x_values[0] - rtmax if peak_x_values[0] > rtmax else None\n",
    "        \n",
    "                    else:\n",
    "                        peak_area = measured_RT = fwhm = rt_error = None\n",
    "                   \n",
    "                else:\n",
    "                    peak_area = peak_area_alt = fwhm = rtmin = rtmax = measured_RT = rt_error = None\n",
    "        \n",
    "                analysis_df_dict = {'filename': {0: group_name[0]},\n",
    "                                    'query_name': {0: group_name[1]},\n",
    "                                    'query': {0: group_name[2]},\n",
    "                                    'peak_area': {0: peak_area},\n",
    "                                    'peak_area_alt': {0: peak_area_alt},\n",
    "                                    'fwhm': {0: fwhm},\n",
    "                                    'rtmin': {0: rtmin},\n",
    "                                    'rtmax': {0: rtmax},\n",
    "                                    'measured_RT': {0: measured_RT},\n",
    "                                    'rt_error': {0: rt_error},\n",
    "                                    'datapoint_count': {0: len(grouped_df)}}\n",
    "                \n",
    "                analysis_df = pd.DataFrame(data=analysis_df_dict)\n",
    "                ms1_analysis_df_list.append(analysis_df)\n",
    "        \n",
    "    if ms1_analysis_df_list:\n",
    "        ms1_analysis_df = pd.concat(ms1_analysis_df_list)\n",
    "        # ms1_analysis_df.to_csv(\"SpectraSpectre_Output/\"+timestr+\"/\"+timestr+\"_ms1_analysis_df.csv\")\n",
    "    \n",
    "    return ms1_analysis_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7f436-de16-42bd-b428-13150486c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"merge ms1 query dataframe with ms1 analysis dataframe\"\"\"\n",
    "\n",
    "def ms1_query_analysis_merge(ms1_analysis_df, ms1_query_df):\n",
    "    if not ms1_analysis_df.empty and not ms1_query_df.empty:\n",
    "        ms1_analysis_df = pd.merge(ms1_analysis_df, ms1_query_df.rename(columns={'name': 'query_name'}), on='query_name', how='inner', suffixes=('', '_duplicate'))\n",
    "        ms1_analysis_df = ms1_analysis_df.drop(columns=[col for col in ms1_analysis_df.columns if \"_duplicate\" in col])\n",
    "    return ms1_analysis_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808589e-cb16-4bd7-b3dd-19f5925ec211",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MS1 Peak Validity and QC Check\"\"\"\n",
    "\n",
    "def ms1_validity_and_QC(ms1_analysis_df, fwhm_thresh = 1, rt_thresh = 0.1, datapoint_thresh = 5, abundance_thresh_ms1 = 10, impute_ms1_data = True):\n",
    "    if not ms1_analysis_df.empty:\n",
    "        # peak_condition = (ms1_analysis_df['fwhm'] < fwhm_thresh) & (abs(ms1_analysis_df['rt_error']) <= rt_thresh) & (ms1_analysis_df['datapoint_count'] >= datapoint_thresh)\n",
    "        peak_condition = (\n",
    "            (ms1_analysis_df['fwhm'] < fwhm_thresh) &\n",
    "            (ms1_analysis_df['rt_error'].apply(lambda x: np.abs(x) <= rt_thresh if x is not None else False)) &\n",
    "            (ms1_analysis_df['datapoint_count'] >= datapoint_thresh))\n",
    "        ms1_analysis_df['peak_valid'] = np.where(peak_condition, True, False)\n",
    "        # ms1_analysis_df['abundance_error'] = ((ms1_analysis_df['peak_area'] - ms1_analysis_df.get('abundance')) / ms1_analysis_df.get('abundance')) * 100\n",
    "        if 'abundance' in ms1_analysis_df.columns:\n",
    "            ms1_analysis_df['abundance_error'] = np.where(\n",
    "                (ms1_analysis_df['peak_area'].notnull()) & (ms1_analysis_df['abundance'].notnull()) & (ms1_analysis_df['abundance'] != 0),\n",
    "                ((ms1_analysis_df['peak_area'] - ms1_analysis_df['abundance']) / ms1_analysis_df['abundance']) * 100, None)\n",
    "            abundance_condition = ms1_analysis_df['abundance_error'].apply(\n",
    "                lambda x: np.abs(x) <= abundance_thresh_ms1 if x is not None else False)\n",
    "            ms1_analysis_df['abundance_valid'] = np.where(abundance_condition, True, False)\n",
    "        else:\n",
    "             ms1_analysis_df['abundance_error'] = ms1_analysis_df['abundance_valid'] = None\n",
    "        \n",
    "    return ms1_analysis_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5cfc95-7c65-430e-8fbd-0362703c87eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Impute MS1 function\"\"\"\n",
    "\n",
    "def impute_ms1(ms1_analysis_df, c1='query_name', c3='filename', c4='peak_valid', c5='abundance_valid'):\n",
    "    result_xdf = pd.DataFrame()\n",
    "    if not ms1_analysis_df.empty:\n",
    "        # Get unique values of c1 and c2\n",
    "        unique_c1 = ms1_analysis_df[c1].unique()\n",
    "        \n",
    "        # Generate all combinations of c1, c2, and c3\n",
    "        all_combinations = list(product(unique_c1, ms1_analysis_df[c3].unique()))\n",
    "        \n",
    "        # Create a DataFrame with all combinations\n",
    "        result_xdf = pd.DataFrame(all_combinations, columns=[c1, c3])\n",
    "        \n",
    "        # Merge with the original DataFrame to get the 'Value' column\n",
    "        result_xdf = pd.merge(result_xdf, ms1_analysis_df, on=[c1, c3], how='left', indicator=True)\n",
    "        # Create 'imputed' column based on the '_merge' indicator\n",
    "        result_xdf['imputed'] = result_xdf['_merge'].eq('left_only')\n",
    "        \n",
    "        # Drop the '_merge' column\n",
    "        result_xdf.drop('_merge', axis=1, inplace=True)\n",
    "        result_xdf[c4] = result_xdf[c4].fillna(False)\n",
    "        result_xdf[c5] = result_xdf[c5].fillna(False)\n",
    "    \n",
    "    return result_xdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2b1b3-a135-40bc-849b-9554c6e677c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Export MS1 analysis dataframe\"\"\"\n",
    "\n",
    "def export_ms1_analysis_df(ms1_analysis_df, data_directory, timestr):\n",
    "    if not ms1_analysis_df.empty:\n",
    "        ms1_analysis_df.to_csv(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_analysis_df.csv\")\n",
    "        sys.stdout.write(f\"\\nCreated ms1_analysis_df and exported as csv.\") \n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        sys.stdout.write(f\"\\nms1_analysis_df empty. Not writing to csv.\")\n",
    "        sys.stdout.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc3838-353f-4a7e-8db2-d58fc395bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"RT Analysis MS1\"\"\"\n",
    "\n",
    "def rt_analysis_ms1(ms1_analysis_df, data_directory, timestr):\n",
    "    if not ms1_analysis_df.empty:\n",
    "        df = ms1_analysis_df.copy()\n",
    "        df.loc[~df['peak_valid'], 'measured_RT'] = None\n",
    "        pivot_df = df.pivot(index='query_name', columns='filename', values='measured_RT')\n",
    "        original_columns = pivot_df.columns.tolist()\n",
    "        \n",
    "        # Calculate the average of each row based on original 'measured_RT' values\n",
    "        pivot_df['average'] = pivot_df[original_columns].mean(axis=1)\n",
    "        \n",
    "        # Calculate the standard deviation of each row based on original 'measured_RT' values\n",
    "        pivot_df['std_dev'] = pivot_df[original_columns].std(axis=1)\n",
    "        \n",
    "        # Add a new column with the RSD percentage of each row\n",
    "        pivot_df['RSD_%'] = (pivot_df['std_dev'] / pivot_df['average']) * 100\n",
    "        \n",
    "        # Add a new column 'valid_%' with the percentage of valid (non-None) values\n",
    "        pivot_df['valid_%'] = (pivot_df[original_columns].count(axis=1) / len(original_columns)) * 100\n",
    "        \n",
    "        if not pivot_df.empty:\n",
    "            pivot_df.to_csv(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_RT_analysis_df.csv\")\n",
    "            sys.stdout.write(f\"\\nCreated ms1_RT_analysis_df and exported as csv.\") \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "    else:\n",
    "        sys.stdout.write(f\"\\nms1_analysis_df empty. Not doing rt_analysis_ms1.\")\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29511eed-e5ab-4e73-8c62-6c991412a841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Summary MS1 traces\"\"\"\n",
    "\n",
    "def summary_ms1_traces(raw_df_ms1, data_directory, timestr):\n",
    "    if not raw_df_ms1.empty:\n",
    "        with PdfPages(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_summary_traces.pdf\") as pdf:\n",
    "            # Iterate through unique 'query_name' groups\n",
    "            for i, (frame_group, frame_data) in enumerate(raw_df_ms1.groupby(['query_name', 'query'])):\n",
    "                rtmin = extract_rtmin_value(frame_group[1])\n",
    "                rtmax = extract_rtmax_value(frame_group[1])\n",
    "            \n",
    "                # Create subplots based on 'c' groups within each 'd' group\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt_title = frame_group[0]\n",
    "                plt.suptitle(plt_title, y=0.92)\n",
    "                left_ax = plt.subplot(121)\n",
    "                right_ax = plt.subplot(122)\n",
    "                \n",
    "                colors_plot = plt.rcParams[\"axes.prop_cycle\"]()\n",
    "                \n",
    "                # Iterate through unique 'filename' groups within each 'query_name' group\n",
    "                for group_name, group_data in frame_data.groupby('filename'):\n",
    "                    c = next(colors_plot)[\"color\"]\n",
    "                    # Plot 'rt' versus 'i' for each 'filename' group within the 'query_name' group\n",
    "                    group_data.plot(x='rt', y='i', kind='line', label=group_name, ax=left_ax, color=c, style='.-')\n",
    "                    left_ax.set_ylabel('intensity')  # Add y-axis label\n",
    "            \n",
    "                    group_data.plot(x='rt', y='i', kind='line', label=group_name, ax=right_ax, color=c, style='.-')\n",
    "                    # group_data.plot(x='rt', y='i', kind='scatter', label=group_name, ax=right_ax, color=c)\n",
    "        \n",
    "                    # group_data.plot(x='rt', y='fit', kind='line', label=group_name, ax=right_ax, color=c, style='--', alpha=0.5)\n",
    "                    # group_data.plot(x='rt', y='fit', kind='line', label=group_name, ax=right_ax, color=c)\n",
    "        \n",
    "                    range_rt = rtmax-rtmin\n",
    "                    right_ax.set_xlim(rtmin-range_rt, rtmax+range_rt)\n",
    "            \n",
    "                # left_ax.set_title(f'{frame_group[0]}')\n",
    "                if len(left_ax.get_lines()) >5:\n",
    "                    left_ax.get_legend().remove()\n",
    "                \n",
    "                # right_ax.set_title(f'{frame_group[0]} (RT)')\n",
    "                right_ax.get_legend().remove()\n",
    "            \n",
    "                right_ax.axvline(x=rtmin, color='black', linestyle=':')\n",
    "                right_ax.axvline(x=rtmax, color='black', linestyle=':')\n",
    "            \n",
    "                plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        \n",
    "                pdf.savefig()\n",
    "                if not os.path.exists(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_summary_traces/\"):\n",
    "                    os.makedirs(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_summary_traces/\")\n",
    "                plt.savefig(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_summary_traces/\"+plt_title+'.png')\n",
    "                # plt.show()\n",
    "                plt.close()\n",
    "        sys.stdout.write(f\"\\nCreated ms1_summary_traces.\") \n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0f262-c581-4ab8-8f7f-ab2056c536ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Summary MS1 Areas\"\"\"\n",
    "\n",
    "def summary_ms1_areas(ms1_analysis_df, data_directory, timestr, abundance_thresh_ms1 = 10):\n",
    "    if not ms1_analysis_df.empty:\n",
    "        with PdfPages(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_summary_areas.pdf\") as pdf:\n",
    "            for i, (frame_group, frame_data) in enumerate(ms1_analysis_df.groupby(['query_name'])):\n",
    "                colors_plot = plt.rcParams[\"axes.prop_cycle\"]()\n",
    "                \n",
    "                frame_data.loc[frame_data['peak_valid'] == False, 'peak_area'] = np.nan\n",
    "                \n",
    "                plt.figure(figsize=(5, 5))\n",
    "                plt_title = frame_group[0]\n",
    "                plt.title(plt_title)\n",
    "                frame_data['filename_short'] = '..' + frame_data['filename'].str[-15:-5]\n",
    "                frame_data['filename_short_Index'] = range(1, len(frame_data) + 1)\n",
    "                # Iterate through unique 'filename' groups within each 'query_name' group\n",
    "                for group_name, group_data in frame_data.groupby('filename'):\n",
    "                    c = next(colors_plot)[\"color\"]\n",
    "                    # Plot 'rt' versus 'i' for each 'filename' group within the 'query_name' group\n",
    "                    \n",
    "                    plt.bar(group_data['filename_short_Index'], group_data['peak_area'])\n",
    "        \n",
    "                plt.xticks(frame_data['filename_short_Index'])\n",
    "                plt.gca().set_xticklabels(frame_data['filename_short'])\n",
    "                plt.xlim(frame_data['filename_short_Index'].min() - 1, frame_data['filename_short_Index'].max() + 1)\n",
    "        \n",
    "                \n",
    "                if 'abundance' in frame_data.columns:\n",
    "                    frame_abundance = frame_data['abundance'].mean()\n",
    "                    plt.axhline(y=(frame_abundance+(frame_abundance*(abundance_thresh_ms1/100))), color='black', linestyle='--')\n",
    "                    plt.axhline(y=(frame_abundance-(frame_abundance*(abundance_thresh_ms1/100))), color='black', linestyle='--')\n",
    "                plt.tick_params(axis='x', rotation=90)\n",
    "                plt.ylabel('peak area')\n",
    "                plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        \n",
    "                pdf.savefig()\n",
    "                if not os.path.exists(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_summary_areas/\"):\n",
    "                    os.makedirs(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_summary_areas/\")\n",
    "                plt.savefig(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_summary_areas/\"+plt_title+'.png')\n",
    "                # plt.show()\n",
    "                plt.close()\n",
    "        sys.stdout.write(f\"\\nCreated ms1_summary_areas.\") \n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957904ce-3a6c-4556-ba8f-83612b02998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analysis MS2\"\"\"\n",
    "\n",
    "def analysis_ms2(raw_df_ms2, ms2_query_df, abundance_thresh_ms2 = 20):\n",
    "    ms2_analysis_df = pd.DataFrame()\n",
    "    ms2_analysis_df_list = []\n",
    "    \n",
    "    if not raw_df_ms2.empty:\n",
    "        for group_name, grouped_df in raw_df_ms2.groupby(['filename', 'query_name', 'query', 'collision_type_energy']):\n",
    "            grouped_df['i'].fillna(0, inplace=True)\n",
    "    \n",
    "            if not grouped_df.empty and len(grouped_df) > 0:\n",
    "                \n",
    "                grouped_df = grouped_df.loc[grouped_df['i'].idxmax()].to_frame().T\n",
    "                grouped_df['i'] = pd.to_numeric(grouped_df['i'])\n",
    "                grouped_df['measured_RT'] = grouped_df.loc[grouped_df['i'].idxmax()]['rt']\n",
    "                grouped_df['scan'] = grouped_df.iloc[0][\"scan\"]\n",
    "                grouped_df['collision_type_energy'] = grouped_df.iloc[0][\"collision_type_energy\"]\n",
    "    \n",
    "                ms2_analysis_df_list.append(grouped_df)\n",
    "                    \n",
    "            else:\n",
    "                #TODO check what shows up here\n",
    "                i = measured_RT = scan = collision_type_energy  = None\n",
    "    \n",
    "    if ms2_analysis_df_list:\n",
    "        ms2_analysis_df = pd.concat(ms2_analysis_df_list).reset_index(drop=True)\n",
    "\n",
    "    if not ms2_analysis_df.empty and not ms2_query_df.empty:\n",
    "        ms2_analysis_df = pd.merge(ms2_analysis_df, ms2_query_df.rename(columns={'name': 'query_name'}), on='query_name', how='inner', suffixes=('', '_duplicate'))\n",
    "        ms2_analysis_df = ms2_analysis_df.drop(columns=[col for col in ms2_analysis_df.columns if \"_duplicate\" in col])\n",
    "    \n",
    "    # if not ms2_analysis_df.empty and ms2_analysis_df.get('abundance'):\n",
    "    if not ms2_analysis_df.empty and 'abundance' in ms2_analysis_df.columns:\n",
    "        ms2_analysis_df['abundance_error'] = np.where(\n",
    "            (ms2_analysis_df['i'].notnull()) & (ms2_analysis_df['abundance'].notnull()) & (ms2_analysis_df['abundance'] != 0),\n",
    "            ((ms2_analysis_df['i'] - ms2_analysis_df['abundance']) / ms2_analysis_df['abundance']) * 100, None)\n",
    "        abundance_condition = ms2_analysis_df['abundance_error'].apply(\n",
    "            lambda x: np.abs(x) <= abundance_thresh_ms2 if x is not None else False)\n",
    "        ms2_analysis_df['abundance_valid'] = np.where(abundance_condition, True, False)\n",
    "    else:\n",
    "         ms2_analysis_df['abundance_error'] = ms2_analysis_df['abundance_valid'] = None\n",
    "\n",
    "    return ms2_analysis_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b8d24-549e-4c4d-a667-5de48c247616",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Impute MS2 function\"\"\"\n",
    "\n",
    "def impute_ms2(ms2_analysis_df, c1='query_name', c2='collision_type_energy', c3='filename', c4='query'):\n",
    "    result_xdf = pd.DataFrame()\n",
    "    if not ms2_analysis_df.empty:\n",
    "        # Get unique values of c1 and c2\n",
    "        unique_c1 = ms2_analysis_df[c1].unique()\n",
    "        unique_c2 = ms2_analysis_df[c2].unique()\n",
    "    \n",
    "        # Map c1 to c4\n",
    "        c1_to_c4 = dict(zip(ms2_analysis_df[c1], ms2_analysis_df[c4]))\n",
    "        \n",
    "        # Generate all combinations of c1, c2, and c3\n",
    "        all_combinations = list(product(unique_c1, unique_c2, ms2_analysis_df[c3].unique()))\n",
    "        \n",
    "        # Create a DataFrame with all combinations\n",
    "        result_xdf = pd.DataFrame(all_combinations, columns=[c1, c2, c3])\n",
    "    \n",
    "        # Map c4 values based on c1\n",
    "        result_xdf[c4] = result_xdf[c1].map(c1_to_c4)\n",
    "        \n",
    "        # Merge with the original DataFrame to get the 'Value' column\n",
    "        result_xdf = pd.merge(result_xdf, ms2_analysis_df, on=[c1, c2, c3, c4], how='left', indicator=True)\n",
    "        result_xdf['imputed'] = result_xdf['_merge'].eq('left_only')\n",
    "        result_xdf.drop('_merge', axis=1, inplace=True)\n",
    "        \n",
    "    return result_xdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db61fe0-dee6-4b4a-90c6-d859c9615eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"export_ms2_analysis_df\"\"\"\n",
    "\n",
    "def export_ms2_analysis_df(ms2_analysis_df, data_directory, timestr):\n",
    "    if not ms2_analysis_df.empty:\n",
    "        ms2_analysis_df.to_csv(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_analysis_df.csv\")\n",
    "        sys.stdout.write(f\"\\nCreated ms2_analysis_df and exported as csv.\")\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        sys.stdout.write(f\"\\nms2_analysis_df empty. Not writing to csv.\")\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e81bb-b98b-4743-b463-0f80eeda315a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Cluster Plot MS2\"\"\"\n",
    "\n",
    "def cluster_plot_ms2(ms2_analysis_df, data_directory, timestr):\n",
    "    if not ms2_analysis_df.empty:\n",
    "        with PdfPages(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_cluster_plots.pdf\") as pdf:\n",
    "            for group_name, grouped_df in ms2_analysis_df.groupby('collision_type'):\n",
    "                for g_group_name, g_grouped_df in grouped_df.groupby(['query_name', 'query']):\n",
    "                    if not grouped_df.empty and len(grouped_df) > 0:\n",
    "                        pivot_df = g_grouped_df.pivot(index='energy', columns='filename', values='i')\n",
    "                        pivot_df.plot(kind='bar', figsize=(10, 6))\n",
    "                        plt.xlabel('Filename')\n",
    "                        plt.ylabel('Intensity')\n",
    "                        plt_title = f'{g_group_name[0]}_{group_name}'\n",
    "                        plt_title_w_query = f'{g_group_name[0]}, {group_name}, {g_group_name[1]}'\n",
    "                        wrapped_title = textwrap.fill(plt_title_w_query, width=100)\n",
    "                        plt.title(wrapped_title, fontsize=8)\n",
    "                        # plt.title(f'{g_group_name[1]}', fontsize=8)\n",
    "                        pdf.savefig()\n",
    "                        \n",
    "                        if not os.path.exists(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_cluster_plots/\"):\n",
    "                            os.makedirs(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_cluster_plots/\")\n",
    "                        plt.savefig(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_cluster_plots/\"+plt_title+'.png')\n",
    "                        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62eb07b-65eb-426c-9077-682aa0903900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cluster Plot MS2 alt\"\"\"\n",
    "\n",
    "def cluster_plot_ms2_alt(ms2_analysis_df, data_directory, timestr):\n",
    "    if not ms2_analysis_df.empty:\n",
    "        with PdfPages(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_cluster_plots_alt.pdf\") as pdf:\n",
    "            for group_name, grouped_df in ms2_analysis_df.groupby('collision_type'):\n",
    "                for g_group_name, g_grouped_df in grouped_df.groupby(['query_name', 'query']):\n",
    "                    if not grouped_df.empty and len(grouped_df) > 0:\n",
    "                        pivot_df = g_grouped_df.pivot(index='filename', columns='energy', values='i')\n",
    "                        pivot_df.plot(kind='bar', figsize=(10, 6))\n",
    "                        plt.xlabel('Filename')\n",
    "                        plt.ylabel('Intensity')\n",
    "                        plt_title = f'{g_group_name[0]}_{group_name}'\n",
    "                        plt_title_w_query = f'{g_group_name[0]}, {group_name}, {g_group_name[1]}'\n",
    "                        wrapped_title = textwrap.fill(plt_title_w_query, width=100)\n",
    "                        plt.title(wrapped_title, fontsize=8)\n",
    "                        # plt.title(f'{g_group_name[1]}', fontsize=8)\n",
    "                        pdf.savefig()\n",
    "                        \n",
    "                        if not os.path.exists(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_cluster_plots_alt/\"):\n",
    "                            os.makedirs(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_cluster_plots_alt/\")\n",
    "                        plt.savefig(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_cluster_plots_alt/\"+plt_title+'.png')\n",
    "                        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635c8c4-bc01-4461-87dc-e74914a8a163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Data Plot MS2\"\"\"\n",
    "\n",
    "def summary_ms2(ms2_analysis_df, data_directory, timestr, abundance_thresh_ms2=20):\n",
    "    if not ms2_analysis_df.empty:\n",
    "        with PdfPages(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_summary_plots.pdf\") as pdf:\n",
    "            for i, (frame_group, frame_data) in enumerate(ms2_analysis_df.groupby(['query_name', 'collision_type_energy'])):\n",
    "                colors_plot = plt.rcParams[\"axes.prop_cycle\"]()\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                if frame_group[1] == 'NA':\n",
    "                    plt_title = frame_group[0]\n",
    "                else:\n",
    "                    plt_title = frame_group[0] + '_' + frame_group[1]\n",
    "                plt.suptitle(plt_title, y=0.92)\n",
    "                left_ax = plt.subplot(121)\n",
    "                right_ax = plt.subplot(122)\n",
    "                \n",
    "                frame_data['filename_short'] = '..' + frame_data['filename'].str[-15:-5]\n",
    "                frame_data['filename_short_Index'] = range(1, len(frame_data) + 1)\n",
    "                for group_name, group_data in frame_data.groupby('filename'):\n",
    "                    c = next(colors_plot)[\"color\"]\n",
    "                    group_data.plot(x='rt', y='i', kind='scatter', label=group_name, ax=left_ax, color=c, style='.-')\n",
    "                    left_ax.set_ylabel('intensity')  # Add y-axis label\n",
    "                    right_ax.bar(group_data['filename_short_Index'], group_data['i'])\n",
    "        \n",
    "                right_ax.set_xticks(frame_data['filename_short_Index'])\n",
    "                right_ax.set_xticklabels(frame_data['filename_short'])\n",
    "                right_ax.set_xlim(frame_data['filename_short_Index'].min() - 1, frame_data['filename_short_Index'].max() + 1)\n",
    "\n",
    "                if 'abundance' in frame_data.columns:\n",
    "                    frame_abundance = frame_data['abundance'].mean()\n",
    "                    plt.tick_params(axis='x', rotation=90)\n",
    "                    plt.axhline(y=(frame_abundance+(frame_abundance*(abundance_thresh_ms2/100))), color='black', linestyle='--')\n",
    "                    plt.axhline(y=(frame_abundance-(frame_abundance*(abundance_thresh_ms2/100))), color='black', linestyle='--')\n",
    "                \n",
    "                if len(left_ax.get_lines()) >4:\n",
    "                    left_ax.get_legend().remove()\n",
    "        \n",
    "                right_ax.tick_params(axis='x', rotation=90)\n",
    "                left_ax.axhline(y=0, color='black', linestyle='--')\n",
    "                right_ax.set_xlabel('')\n",
    "                right_ax.set_ylabel('')\n",
    "            \n",
    "                plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        \n",
    "                pdf.savefig()\n",
    "                if not os.path.exists(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_summary_plots/\"):\n",
    "                    os.makedirs(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_summary_plots/\")\n",
    "                plt.savefig(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_summary_plots/\"+plt_title+'.png')\n",
    "                # plt.show()\n",
    "                plt.close()\n",
    "        sys.stdout.write(f\"\\nCreated ms2_summary_plots.\") \n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae2600-7e4c-4fc6-8efc-28683c2c50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Save MS2 Scan\"\"\"\n",
    "\n",
    "def save_ms2_scans_old(ms2_analysis_df, data_directory, timestr):\n",
    "    if not ms2_analysis_df.empty:\n",
    "        output_dir = os.path.join(data_directory, \"SpectraSpectre_Output\", timestr, \"scans\")\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        for filename_group_name, filename_group_df in ms2_analysis_df.groupby([\"filename\"]):\n",
    "            reader = mzml.MzML(os.path.join(data_directory, filename_group_name[0]))\n",
    "            filename_group_df = filename_group_df.dropna(subset=['scan'])\n",
    "            pdf_filename = os.path.join(output_dir, f\"{filename_group_name[0]}_scans.pdf\")\n",
    "\n",
    "            with PdfPages(pdf_filename) as pdf:\n",
    "                for group_name, group_df in filename_group_df.groupby([\"query_name\", \"collision_type_energy\"]):\n",
    "                    for index, row in group_df.iterrows():\n",
    "                        scan_num = int(row['scan']) - 1\n",
    "                        spectrum = reader[scan_num]\n",
    "                        mz, intensity = spectrum['m/z array'], spectrum['intensity array']\n",
    "                        height_min = np.max(intensity) / 20\n",
    "                        peaks, properties = find_peaks(intensity, height=height_min, prominence=height_min, distance=20)\n",
    "\n",
    "                        top_peaks_indices = np.argsort(properties[\"prominences\"])[-5:]\n",
    "\n",
    "                        plt.figure(figsize=(10, 6))\n",
    "                        plt.plot(mz, intensity, label='Spectrum')\n",
    "                        plt.scatter(mz[peaks[top_peaks_indices]], intensity[peaks[top_peaks_indices]], color='red', label='Top Peaks')\n",
    "\n",
    "                        for peak_idx in top_peaks_indices:\n",
    "                            plt.annotate(f'{mz[peaks[peak_idx]]:.2f}', \n",
    "                                         xy=(mz[peaks[peak_idx]], intensity[peaks[peak_idx]]), \n",
    "                                         xytext=(0, 5), \n",
    "                                         textcoords='offset points', \n",
    "                                         ha='center', fontsize=8)\n",
    "\n",
    "                        plt.xlabel('m/z')\n",
    "                        plt.ylabel('Intensity')\n",
    "                        plot_title = f'{group_name[0]}, {group_name[1]}, scan:{scan_num}'\n",
    "                        plt.title(plot_title, fontsize=8)\n",
    "                        pdf.savefig()\n",
    "                        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4064e2-dfb4-4878-80d3-30007169a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Save MS2 Scan\"\"\"\n",
    "\n",
    "def save_ms2_scans(ms2_analysis_df, data_directory, timestr):\n",
    "    if not ms2_analysis_df.empty:\n",
    "        output_dir = os.path.join(data_directory, \"SpectraSpectre_Output\", timestr, \"scans\")\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        for filename_group_name, filename_group_df in ms2_analysis_df.groupby([\"filename\", \"collision_type\"]):\n",
    "            reader = mzml.MzML(os.path.join(data_directory, filename_group_name[0]))\n",
    "            filename_group_df = filename_group_df.dropna(subset=['scan'])\n",
    "            pdf_filename = os.path.join(output_dir, f\"{filename_group_name[0]}_{filename_group_name[1]}_scans.pdf\")\n",
    "\n",
    "            with PdfPages(pdf_filename) as pdf:\n",
    "                for group_name, group_df in filename_group_df.groupby([\"query_name\", \"energy\"]):\n",
    "                    for index, row in group_df.iterrows():\n",
    "                        scan_num = int(row['scan']) - 1\n",
    "                        spectrum = reader[scan_num]\n",
    "                        mz, intensity = spectrum['m/z array'], spectrum['intensity array']\n",
    "                        height_min = np.max(intensity) / 20\n",
    "                        peaks, properties = find_peaks(intensity, height=height_min, prominence=height_min, distance=20)\n",
    "\n",
    "                        top_peaks_indices = np.argsort(properties[\"prominences\"])[-5:]\n",
    "\n",
    "                        plt.figure(figsize=(10, 6))\n",
    "                        plt.plot(mz, intensity, label='Spectrum')\n",
    "                        plt.scatter(mz[peaks[top_peaks_indices]], intensity[peaks[top_peaks_indices]], color='red', label='Top Peaks')\n",
    "\n",
    "                        for peak_idx in top_peaks_indices:\n",
    "                            plt.annotate(f'{mz[peaks[peak_idx]]:.2f}', \n",
    "                                         xy=(mz[peaks[peak_idx]], intensity[peaks[peak_idx]]), \n",
    "                                         xytext=(0, 5), \n",
    "                                         textcoords='offset points', \n",
    "                                         ha='center', fontsize=8)\n",
    "\n",
    "                        plt.xlabel('m/z')\n",
    "                        plt.ylabel('Intensity')\n",
    "                        plot_title = f'{group_name[0]}, Energy: {group_name[1]}, scan:{scan_num}'\n",
    "                        plt.title(plot_title, fontsize=8)\n",
    "                        pdf.savefig()\n",
    "                        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc37c98-395d-4254-92a5-9c8725ce843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize ReportLab\"\"\"\n",
    "\n",
    "def on_page(canvas, doc):\n",
    "    page_num = canvas.getPageNumber()\n",
    "    canvas.drawCentredString(A4[0]/2, 50, str(page_num))\n",
    "\n",
    "def on_page_landscape(canvas, doc):\n",
    "  return on_page(canvas, doc)\n",
    "\n",
    "def fig2image(f):\n",
    "    buf = io.BytesIO()\n",
    "    f.savefig(buf, format='png', dpi=300)\n",
    "    buf.seek(0)\n",
    "    x, y = f.get_size_inches()\n",
    "    return Image(buf, x * inch, y * inch)\n",
    "\n",
    "def df2table(df):\n",
    "    return Table(\n",
    "      [[Paragraph(col) for col in df.columns]] + df.values.tolist(), \n",
    "      style=[\n",
    "        ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n",
    "        ('FONTSIZE', (0, 0), (-1, -1), 8),\n",
    "        ('LINEBELOW',(0,0), (-1,0), 1, colors.black),\n",
    "        ('INNERGRID', (0,0), (-1,-1), 0.25, colors.black),\n",
    "        ('BOX', (0,0), (-1,-1), 1, colors.black),\n",
    "        ('ROWBACKGROUNDS', (0,0), (-1,-1), [colors.lightgrey, colors.white])],\n",
    "      hAlign = 'LEFT')\n",
    "\n",
    "def get_image(path, width):\n",
    "    img = utils.ImageReader(path)\n",
    "    iw, ih = img.getSize()\n",
    "    aspect = ih / float(iw)\n",
    "    return Image(path, width=width, height=(width * aspect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a78c56-e192-46b1-bf52-626a3cb03c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MS1 build reportlab doc\"\"\"\n",
    "\n",
    "def reportlab_ms1(ms1_analysis_df, data_directory, timestr):\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    padding = dict(\n",
    "      leftPadding=24, \n",
    "      rightPadding=24,\n",
    "      topPadding=24,\n",
    "      bottomPadding=24)\n",
    "    \n",
    "    portrait_frame = Frame(0, 0, *A4, **padding)\n",
    "    landscape_frame = Frame(0, 0, *landscape(A4), **padding)\n",
    "    \n",
    "    portrait_template = PageTemplate(\n",
    "      id='portrait', \n",
    "      frames=portrait_frame,\n",
    "      onPage=on_page)\n",
    "    \n",
    "    landscape_template = PageTemplate(\n",
    "      id='landscape', \n",
    "      frames=landscape_frame, \n",
    "      onPage=on_page_landscape)\n",
    "\n",
    "    if not ms1_analysis_df.empty:\n",
    "        for group_name, grouped_df in ms1_analysis_df.groupby(['query_name', 'query']):\n",
    "        \n",
    "            pdfname = data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_reports/\"+group_name[0]+\"_ms1_report.pdf\"\n",
    "            doc = BaseDocTemplate(\n",
    "              pdfname,\n",
    "              pageTemplates=[\n",
    "                portrait_template,\n",
    "                landscape_template])\n",
    "        \n",
    "            image_path1 = data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_summary_traces/\"+group_name[0]+\".png\"\n",
    "            image_path2 = data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_summary_areas/\"+group_name[0]+\".png\"\n",
    "        \n",
    "            grouped_df1 = grouped_df[['filename', 'peak_area', 'fwhm', 'abundance_valid']]\n",
    "            grouped_df2 = grouped_df[['filename', 'measured_RT', 'rt_error', 'peak_valid']]\n",
    "        \n",
    "            story = [\n",
    "                Paragraph('MS1 Report: '+str(group_name[0]), styles['Heading1']),\n",
    "                Paragraph(str(group_name[1]), styles['Heading2']),\n",
    "                get_image(image_path1, width=7*inch),\n",
    "                get_image(image_path2, width=5*inch),\n",
    "                PageBreak(),\n",
    "                df2table(grouped_df1),\n",
    "                PageBreak(),\n",
    "                df2table(grouped_df2)\n",
    "            ]\n",
    "        \n",
    "            if not os.path.exists(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_reports/\"):\n",
    "                    os.makedirs(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms1_reports/\")\n",
    "                \n",
    "            doc.build(story)\n",
    "        sys.stdout.write(f\"\\nCreated ms1_reports.\") \n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92a938-50ce-44e2-baca-a25d72cb6d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MS2 build reportlab doc. Create\"\"\"\n",
    "\n",
    "def reportlab_ms2(ms2_analysis_df, data_directory, timestr):\n",
    "    \n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    padding = dict(\n",
    "        leftPadding=24,\n",
    "        rightPadding=24,\n",
    "        topPadding=24,\n",
    "        bottomPadding=24)\n",
    "        \n",
    "    portrait_frame = Frame(0, 0, *A4, **padding)\n",
    "    landscape_frame = Frame(0, 0, *landscape(A4), **padding)\n",
    "    \n",
    "    portrait_template = PageTemplate(\n",
    "      id='portrait', \n",
    "      frames=portrait_frame,\n",
    "      onPage=on_page)\n",
    "    \n",
    "    landscape_template = PageTemplate(\n",
    "      id='landscape', \n",
    "      frames=landscape_frame, \n",
    "      onPage=on_page_landscape)\n",
    "\n",
    "    if not ms2_analysis_df.empty:\n",
    "        for group_name, grouped_df in ms2_analysis_df.groupby(['query_name', 'query']):\n",
    "        \n",
    "            story = [Paragraph('MS2 Report: '+str(group_name[0]), styles['Heading1']),\n",
    "                     Paragraph(str(group_name[1]), styles['Heading2'])\n",
    "            ]\n",
    "            \n",
    "            pdfname = data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_reports/\"+group_name[0]+\"_ms2_report.pdf\"\n",
    "            doc = BaseDocTemplate(\n",
    "              pdfname,\n",
    "              pageTemplates=[\n",
    "                portrait_template,\n",
    "                landscape_template])\n",
    "        \n",
    "            for group_name2, grouped_df2 in grouped_df.groupby('collision_type_energy'):    \n",
    "                \n",
    "                image_path1 = data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_summary_plots/\"+group_name[0]+\"_\"+group_name2+\".png\"\n",
    "        \n",
    "                grouped_data = grouped_df2[['filename', 'i', 'rt', 'abundance_error', 'abundance_valid']]\n",
    "        \n",
    "                story = story + (\n",
    "                    [\n",
    "                    # Paragraph(str(group_name[0]), styles['Heading2']),\n",
    "                    Paragraph(str(group_name2), styles['Heading2']),\n",
    "                    # Paragraph(str(group_name[1]), styles['Heading2']),\n",
    "                    get_image(image_path1, width=7*inch),\n",
    "                    df2table(grouped_data),\n",
    "                    PageBreak(),])\n",
    "                \n",
    "            if not os.path.exists(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_reports/\"):\n",
    "                    os.makedirs(data_directory + \"SpectraSpectre_Output/\"+timestr+\"/ms2_reports/\")\n",
    "                \n",
    "            doc.build(story)\n",
    "        sys.stdout.write(f\"\\nCreated ms2_reports.\") \n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa222e-9ca2-4207-bad2-bdd941ebf702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_directory=None, queryfile=None, metadata_file=None, metadata_filename_column=None, metadata_group_columns=None, kegg_path=None, convert_raw=None, msconvertexe=None, cache_setting=None):\n",
    "    if not data_directory and not queryfile:\n",
    "        data_directory, queryfile, metadata_file, metadata_filename_column, metadata_group_columns, kegg_path, convert_raw, msconvertexe, cache_setting = configure_SpectraSpectre()\n",
    "    if data_directory and queryfile:\n",
    "        queries, ms1_query_df, ms2_query_df, query_groups, name_kegg_dict = create_queries(queryfile)\n",
    "        if queries:\n",
    "            convert_raw_files(convert_raw, msconvertexe, data_directory)\n",
    "            file_count = mzml_file_count(data_directory)\n",
    "            raw_df_ms1, raw_df_ms2, filename_groups, timestr = query_files(data_directory, queries)\n",
    "            if not raw_df_ms1.empty:\n",
    "                ms1_analysis_df = analysis_ms1(raw_df_ms1, data_directory, timestr)\n",
    "                ms1_analysis_df = ms1_query_analysis_merge(ms1_analysis_df, ms1_query_df)\n",
    "                ms1_analysis_df = ms1_validity_and_QC(ms1_analysis_df)\n",
    "                ms1_analysis_df = impute_ms1(ms1_analysis_df)\n",
    "                rt_analysis_ms1(ms1_analysis_df, data_directory, timestr)\n",
    "                summary_ms1_traces(raw_df_ms1, data_directory, timestr)\n",
    "                if not ms1_analysis_df.empty:\n",
    "                    export_ms1_analysis_df(ms1_analysis_df, data_directory, timestr)\n",
    "                    summary_ms1_areas(ms1_analysis_df, data_directory, timestr)\n",
    "                    reportlab_ms1(ms1_analysis_df, data_directory, timestr)\n",
    "            if not raw_df_ms2.empty:\n",
    "                ms2_analysis_df = analysis_ms2(raw_df_ms2, ms2_query_df)\n",
    "                ms2_analysis_df = impute_ms2(ms2_analysis_df)\n",
    "                export_ms2_analysis_df(ms2_analysis_df, data_directory, timestr)\n",
    "                cluster_plot_ms2(ms2_analysis_df, data_directory, timestr)      \n",
    "                cluster_plot_ms2_alt(ms2_analysis_df, data_directory, timestr)    \n",
    "                summary_ms2(ms2_analysis_df, data_directory, timestr)\n",
    "                save_ms2_scans(ms2_analysis_df, data_directory, timestr)\n",
    "                reportlab_ms2(ms2_analysis_df, data_directory, timestr)\n",
    "        sys.stdout.write(f\"\\n--------------\\n   Complete   \\n--------------\\n\") \n",
    "        sys.stdout.flush()\n",
    "        time.sleep(1)\n",
    "        run()\n",
    "    else:\n",
    "        sys.stdout.write(f\"\\nNo data_directory and/or queryfile defined\\n\") \n",
    "        sys.stdout.flush()\n",
    "        time.sleep(1)\n",
    "        run()\n",
    "\n",
    "def run():\n",
    "    data_directory, queryfile, metadata_file, metadata_filename_column, metadata_group_columns, kegg_path, convert_raw, msconvertexe, cache_setting = configure_SpectraSpectre()\n",
    "    if data_directory and queryfile:\n",
    "        user_input = input(\"\\n\\n1) Enter 1 to run analysis.\\n2) Enter 2 to reinitialize.\\nAny other input closes.\\nInput: \")\n",
    "        if user_input == '1':\n",
    "            main(data_directory, queryfile, metadata_file, metadata_filename_column, metadata_group_columns, kegg_path, convert_raw, msconvertexe, cache_setting)\n",
    "        elif user_input == '2':\n",
    "            run()\n",
    "        else:\n",
    "            sys.stdout.write(f\"\\nExiting...\\n\") \n",
    "            sys.stdout.flush()\n",
    "            time.sleep(3)\n",
    "            pass\n",
    "    else:\n",
    "        user_input = input(\"\\n\\n2) Enter 2 to reinitialize.\\nAny other input exits.\\nInput: \")\n",
    "        if user_input == '2':\n",
    "            run()\n",
    "        else:\n",
    "            sys.stdout.write(f\"\\nExiting...\\n\") \n",
    "            sys.stdout.flush()\n",
    "            time.sleep(3)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d0cef-d6a6-4611-886a-b730e49615fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f9a51-95c0-4853-8f42-201b5bf2d59d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
